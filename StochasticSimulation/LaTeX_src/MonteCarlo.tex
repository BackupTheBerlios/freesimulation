\chapter{Monte-Carlo Methods}

\section{The M(RT)$^2$ Algorithm}
\label{sec:MRT2}

In a certain sense this chapter is the logical continuation of Chap. xx,
in which we have learned how to sample random variables according to a 
prescribed distribution. We want to discuss an advanced sampling technique
first described in a paper by  Metropolis, Rosenbluth, Rosenbluth, Teller and
Teller, which we will name the M(RT)$^2$--algorithm \cite{Metropolis}. 
In the literature the same algorithms often referred to 
simply as the Metropolis algorithm. The method bears some relation to the
rejection techniques: tentative values that are proposed explicitly may be
accepted or rejected. 

The M(RT)$^2$--algorithm is often used as synonym for Monte Carlo
algorithms. This stems from the fact that it is very simple and, at the same
time, very powerful. It can be used to sample essentially any density
function. The only disadvantage of the method is that the sampling is  correct
only asymptotically and that successive sampled variables are often strongly
correlated. Besides the original paper there are many excellent introductions
to the Monte Carlo algorithm \cite{BinderRepProg, BinderHeermann}. Here, we
will follow a more general approach \cite{Kalos}

It is well--known from statistical physics that the equilibrium properties are
essentially independent from the kinetics of the system. We consider a system
described by a point $X$ in some phase space $\Omega$. The time evolution of
the system is governed by a stochastic transitions. Thus the kinetics of the
system is defined in terms of a conditional transition probability density
function $W(X|Y)$, describing the probability for a system known to be in
state $Y$ to jump in  state near $X$. The quantity $W(X|Y)$ models the
physical process.

A system evolves towards equilibrium and stays there if on average the system
is likely to move from $Y$ into $X$ as to move exactly in the reverse
direction. If the probability density to observe the system in equilibrium is
$P_{\mathrm{eq}}(X)$ then the kinetics must satisfy
\begin{displaymath}
  W(X|Y) P_{\mathrm{eq}}(Y) = W(Y|X) P_{\mathrm{eq}}(X). 
\end{displaymath}
Of course, $W(X|Y) P_{\mathrm{eq}}(Y)$ is the probability for moving from $Y$
to $X$, since it is the  product of the probability of being in $Y$ times the
conditional probability to move from $Y$ to $X$. The above relation is called
detailed balance \index{detailed balance}.

In statistical physics, a model of the system would be defined in terms of 
$W(X|Y)$. A typical task would be the determination of the equilibrium
distribution. It is important to realize that the aim of the 
M(RT)$^2$ algorithm is the opposite! The typical task of a MC simulation, as
we will see in the applications discussed below, is to calculate averages of
some quantities for a \textit{given}  ensemble, i.e., for the canonical one. 
Formulated more abstractly, the task of the M(RT)$^2$ algorithm 
is to find a convenient kinetics, i.e. a $W(X|Y)$, that will equilibrate the
system towards a given  $P_{\mathrm{eq}}(X)$. In this way we are then able to
compute in a simple way ensemble averages. The basic idea of the
M(RT)$^2$ algorithm is to write $W(X|Y)$ in the form
\begin{displaymath}
 W(X|Y) = A(X|Y) T(X|Y), 
\end{displaymath}
where $T(X'|Y)$ is a distribution which proposes the transition from $Y$ to 
$X'$. The $T(X'|Y)$ are normalised such that
\begin{displaymath}
  \int dX T(X|Y) =1.
\end{displaymath}
By comparison of $P_{\mathrm{eq}}(X')$ with $P_{\mathrm{eq}}(Y)$  
and taking into account $T$, the system is either moved to $X'$ (move
accepted)  or the system stays at $Y$ (move rejected). Of course, the
probability of acceptance $A(X'|Y)$ has to be calculated so that the detailed
balance condition is satisfied. In other words:
\begin{displaymath}
  A(X|Y) T(X|Y) P_{\mathrm{eq}}(Y) = A(Y|X) T(Y|X) P_{\mathrm{eq}}(X).
\end{displaymath}
It is helpful to define at this point the quantities
\begin{displaymath}
  q(X|Y) = \frac{ T(Y|X) P_{\mathrm{eq}}(X)}{T(X|Y) P_{\mathrm{eq}}(Y)} \ge 0.
\end{displaymath}
This quantity plays a central role in the calculation of the probabilities to
accept or reject a move. A possibility to define the acceptance probability
$A$ is namely
\begin{displaymath}
  A(X|Y) = \min (1,q(X|Y)).
\end{displaymath}
For a given $P_{\mathrm{eq}}(X)$, when $X$ is assumed to be some
vector in $\mathbf{R}^n$, the M(RT)$^2$ algorithm establishes a random
walk. At each step in the random walk there is a transition density 
$T(X|Y)$ for a move from $Y$ to $X$. Let us now denote the steps of this
random walk by $X_1$, $X_2$, \ldots $X_N$. Each of the $X_i$  is a random
variable with a corresponding probability density $\Phi_1$, \ldots,
$\Phi_N$. As we will demonstrate, by a correct choice of $A$, the asymptotic
distribution of $X$ will be $P_{\mathrm{eq}}(X)$.

We are now in the position to describe the algorithm. We assume that at 
the $n$th step of the random walk the value of $X$ is $X_n$.

(i) Sample a possible next value for $X$, say $X_{n+1}'$ from
    $T(X'_{n+1}|X)$.

(ii) Compute the probability for accepting $X'_{n+1}$. If $q(X'_{n+1}|X) > 1$
then $A(X'_{n+1}|X) = 1$ and $X'_{n+1}$ is accepted. If $q(X'_{n+1}|X) < 1$,
then $A(X'_{n+1}|X) =q(X'_{n+1}|X)$. Thus, with probability   $A(X'_{n+1}|X)$
we accept the move $X_{n+1} = X'_{n+1}$, otherwise we reject the move and
$X_{n+1} = X_{n}$.

Having defined the algorithm let us now look at the asymptotic properties of
the random walk. Let $\Phi_n(X)$ be the distribution of the values $X_n$. The
distribution $\Phi_{n+1}$  is the sum of two contributions. The first one is
the probability of moving in the vicinity of $dX$ of $X$ when we successfully
move from any point $Y$:
\begin{displaymath}
  \int dY A(X|Y) T(X|Y) \Phi_n (Y).
\end{displaymath}
The second one is the probability not to move away from $X$, i.e.,  that a
move out of $X$ is not accepted:
\begin{displaymath}
  \Phi_n(X) \int dY [ 1 - A(Y|X) ] T(Y|X).
\end{displaymath}
Summing up the two contributions we find
\begin{eqnarray}
\label{eq:PhiNRecursion}
  \Phi_{n+1}(X) &=& \int dY A(X|Y) T(X|Y) \Phi_n(Y) \nonumber \\
                && + \Phi_n(X) \int dY [ 1 - A(Y|X) ] T(Y|X),
\end{eqnarray}
which is a recursion relation to the determination of $\Phi_n$. It can be
proven that the dynamical system \index{dynamical system:ergodic} 
generated by the M(RT)$^2$ algorithm is
ergodic: the random walk \index{random walk} starting at $X$ may 
return in the neighbourhood of $X$
but does not do so periodically. According  to a theorem by Feller, if the
random walk defines a system that is ergodic, then there exists an asymptotic
probability density function and is unique if, $P_{eq}(X)$ is a fixed point of
the above recursion
\begin{displaymath}
  \Phi_n(X) = P_{\mathrm{eq}}(X) \Rightarrow \Phi_{n+1}(X) = 
             P_{\mathrm{eq}}(X).
\end{displaymath}
If we now set $\Phi_n(X) = P_{\mathrm{eq}}$ into Eq. (\ref{eq:PhiNRecursion})
we obtain
\begin{eqnarray*}
  \label{eq:PhiNRecursion2}
  \Phi_{n+1}(X) &=& \int dY A(X|Y) T(X|Y) P_{\mathrm{eq}}(Y)  \\
                && + \int dY [ 1 - A(Y|X) ] T(Y|X) P_{\mathrm{eq}}(X). 
\end{eqnarray*}
Because of the detailed balance condition we are left with
\begin{displaymath}
  \Phi_{n+1}(X) = \int T(Y|X) P_{\mathrm{eq}}(X),
\end{displaymath}
and because of the normalisation of the conditional transition probability 
$T$ we finally get
\begin{displaymath}
  \Phi_{n+1}(X) = P_{\mathrm{eq}}(X).
\end{displaymath}
Thus, we have shown that $P_{\mathrm{eq}}(X)$ is the asymptotic distribution of
the random walk.
From the practical point of view, this means that we have always to throw
away $L$ steps of the random walk until the steps are sampled from
$P_{\mathrm{eq}}(X)$. $L$ may be difficult to estimate in advance. However it
is clear that it may be minimised by choosing $\Phi_1(X)$ as close as possible
to $P_{\mathrm{eq}}(X)$.

In a Monte Carlo simulation we want to evaluate quantities of the form
\begin{displaymath}
  G = \frac{\int dX g(X) P_{\mathrm{eq}}(X) }{\int dX P_{\mathrm{eq}}(X) }.
\end{displaymath}
For example, in a Monte Carlo simulation of a many body system $G$ might be
the energy. In view of the above remark, the averaging in the MC simulation
begins only after the initial $L$ steps have been thrown away:
\begin{displaymath}
  G = \sum_{n=L}^{L+N-1} \frac{g(X_n)}{N}.
\end{displaymath}
It is also important to keep in mind that the successive $X_n$ are not
independent and that there may be correlations. Thus the calculated variance
of $G$ will be larger than if the steps were independent. We will return to
this point later. 

A last remark. The form for the probability of accepting a move is not
restricted to the one given above. Another relation which may be used is
\begin{displaymath}
  A'(X|Y) = \frac{q(X|Y)}{1 +q(X|Y) }.
\end{displaymath}

\section{The Ising Model}
A typical application of the M(RT)$^2$--algorithm in statistical physics is
the numerical investigation of the order--disorder transitions. The Ising
model is the easiest model for interacting microscopic degrees of freedom. One
considers a lattice composed of two different types of objects, say $A$ and
$B$.  The objects interact only with their nearest neighbours. The interaction
between object $A$ and object $B$ is $V_{AB}$. The qualitative behaviour of
the system at temperature $T=0$ is easily investigated. We consider two cases:

(i) If $V_{AB} > (V_{AA}+V_{BB})/2$ then the configuration which is
energetically favoured is one in which objects $A$ all neighbour one another
and objects $B$ all neighbour one another. The lattice is split into domains, 
one containing only objects $A$, the other containing only objects $B$.

(ii) If $V_{AB} < (V_{AA}+V_{BB})/2$, the configuration in which objects 
$A$ and $B$ alternate will be energetically favoured.

Obviously the above configurations both belong to ordered states. Raising  the
temperature of the system, the thermal energy $kT$ will tend to randomise the
positions of $A$ and $B$, which at some temperature of the system will behave
completely disordered. The mathematical model of such systems is the Ising
model. It was originally proposed as a model of ferromagnetism 
\index{ferromagnetism}. It has been
also applied successfully to lattice gases \index{lattice gases}, 
binary alloys, melting of DNA,
econophysics \index{econophysics}. 
The Ising model can be solved analytically in 1 and 2
dimensions. No analytical solution in 3 dimensions is known so far. In one
dimension it does not exhibit a phase transition, but in two and three it
does.

We now want to apply the M(RT)$^2$ algorithm to the Ising model. The Ising
model is a simple model of magnetism and is one of the most studied models in
physics. It is known from experiment that the magnetisation of a permanent
magnet diminishes in strength as the magnet is heated, i.e., temperature is 
increased. Above a certain temperature, called the Curie point 
\index{Curie point}, the magnetisation disappears completely. The Ising model
is a simple physical model to explain this phenomenon from a microscopic point
of view. Historically, the model was first proposed by  Lenz in 1920 and it was
investigated by his student Ernst Ising\cite{Ising}, and usually bears 
his name in the literature. The Ising model is the standard model to study
phase transitions. As we will see analytical solutions exist only in 1 and 2
dimensions. In 3 dimensions it has to be studied numerically. The fact that it
can be easily simulated is probably one of the reasons of its great
popularity. The history of the Ising model has been reviewed in Ref. 
\cite{SGBrush}. 

\subsection{The Model}
The microscopic Ising model is based upon th e observation that the atoms of a
magnetic substance are themselves tiny small magnets. The  spontaneous
magnetisation in the bulk of the substance is explained as the result of the
alignment of the "atomic" magnets, usually called  "spins", as a consequence
of their mutual interaction. The assumptions at the basis of the Ising model
are the following ones: (1) the spins are arranged on a regular lattice;
(2) each spin can point only in one of two directions, "up" and "down"; (3)
there exists an interaction energy $-J$ between two neighbouring spins which
point in the same direction, and an energy $J$ if they point in opposite
directions. It is clear that at temperature $T=0$ for positive $J$, the Ising
model will be in a configuration (of lowest energy) with all spins
aligned. This alignment persist, at least partially, up to some nonzero but
finite temperature, the Curie point. We are now in the position to specify
mathematically the Ising model.

We consider a solid of $N$ identical atoms arranged on a regular lattice. Each
atom has a net electron spin $\vec{S}$ and an associated intrinsic magnetic
moment $\vec{\mu}$ 
\begin{displaymath}
  \vec{\mu} = g \mu_0 \vec{S},
\end{displaymath}
where $\mu_0$ is the Bohr magneton \index{Bohr magneton} and $g$ is a factor
of order $O(1)$. In the presence of an external magnetic field $H_0$ in
$z$--direction the Hamiltonian describing the interaction between the magnetic
field and the atoms reads
\begin{displaymath}
  \mathcal{H} = - g \mu_0 \sum_{j=1}^N  \vec{S}_j \cdot \vec{H_0}
              = - g \mu_0 \sum_{j=1}^N S_{jz} H_0,
\end{displaymath}
where the index $j$ runs over all atoms. Of course, atoms interact also with
neighbouring atoms. The magnetic dipole--dipole interaction is generally to 
small to explain ferromagnetism. The dominant interaction is the exchange
interaction between neighbouring spins, which is a direct consequence of the
Pauli exclusion principle \index{Pauli exclusion principle}. Sloppy speaking,
two electrons with parallel spins in neighbouring atoms can not come
arbitrarily near since they occupy the same state. In the opposite case, two
electrons with anti--parallel spins are in different states, and the Pauli
principle does not forbid their approaching. This electrostatic interaction
between neighbouring atoms depends on the relative orientation of the
spins. The exchange interaction \index{exchange interaction}
between two atoms $i$ and $j$ can be written as
\begin{displaymath}
  \mathcal{H}_{ij} = - J \vec{S}_i \vec{S}_j,
\end{displaymath}
where $J$ measures the strength of the interaction. If $J>0$, the interaction
energy is lower if the spins are  parallel. The state of minimal energy is one
which favours configurations in which all the spins are parallel. This is the
situation leading to  ferromagnetism. If $J<0$ it is energetically favourable
for the spins to be antiparallel. 

Since $J$ is a function of the overlapping of the electron wave functions, it
is a function of the distance between the atoms and decreases rapidly. It is
therefore safe to assume that atoms interact only with nearest neighbours and
to regard consequently $J$ as constant. 

According to the assumption of the
Ising model, we consider only components of the spin in the $z$--direction and
we write the exchange interaction as
\begin{displaymath}
  \mathcal{H}_{ij} = -J s_j s_k.
\end{displaymath}
In the above formula we have adopted the  notation $s_i = \pm 1$, where
$s_i$ is the $z$--component of the spin at lattice site $i$. $s_i = +1$ is an 
"up" spin, and $s_i= -1$ is a "down" spin. The total Hamiltonian 
$\mathcal{H}'$ of the exchange interaction reads
\begin{displaymath}
 {\cal{H}}' =  J \sum{\textrm{nn}(ij)} s_j s_k, 
\end{displaymath}
where nn() denotes a sum only over nearest neighbours. Finally, the Ising
Hamiltonian is
\begin{displaymath}
{\cal{H}} = - J \sum_{\textrm{nn}(ij)} s_i s_j - H_0 \sum_i s_i,  
\end{displaymath}
where we have absorbed the factors $\mu_0$ and $g$ into $H_0$.

The problem is now to calculate the thermodynamic functions of the system as,
e.g., the mean magnetic moment $\bar{M}$ as a function of the temperature and
of the external magnetic field. According to the principles of statistical
mechanics, the thermodynamics of the system can be derived from the partition
function 
\begin{displaymath}
  Z = \sum{\textrm{configurations}} \exp(- {\cal{H}}/kT).
\end{displaymath}
The summation has to be taken over all possible spin
configurations. Mathematically, the point is to find a closed expression for
$Z$. If there is a Curie point it will manifest itself in this expression as a
point of non-analyticity in the variable $T$.

In 1925 Ising solved this problem for the 1 dimensional case and found no
Curie point. The existence of a spontaneous magnetisation in 2 and 3
dimensions was established by Peierls \index{Peierls} in 1936, and in 1944 
Onsager \index{Onsager} published the exact solution for the 2 dimensional
case ($H_0=0$) \cite{Onsager}. 
The analytical solution of the Ising model in 3 
dimensions and o f the 2 dimensional Ising model with external field 
remain unknown. The thermodynamic properties can be studied by computer
simulation as we will see shortly. We will not present here the exact
solution of the ising model in 1 and 2 dimensions and refer the reader to the
original literature and to excellent textbooks. Here we prefer, in order to
review some of the basic features of the physics of phase transitions to
recall the mean field approach of Pierre Weiss. 

\subsection{The Mean Field Theory}
In order to understand better the physics behind the Ising model, we want to
treat it in the most simple approximation showing phase transitions and some of
their characteristic features. The basic idea of mean field approximations is
the following one: Pick up a specific spin, say $j$. The interaction of this
spin with the  external magnetic field and the internal field due to the other
spins in the lattice is described by the Hamiltonian
\begin{displaymath}
  H_j = - g \mu_0 H_0 s_j - J s_j \sum_{k=\textrm{nn}(j)}^N s_k.
\end{displaymath}
In order to simplify the second term i the above equation we assume that each
spin interacts with the same internal field due to the other spins in the
lattice. Thus, we replace the sum over all neigbors by the mean value
\begin{displaymath}
  J \langle\sum_{k=\textrm{nn}(j)}^N s_k \rangle \equiv
      g \mu_0 H_m = J qm,
\end{displaymath}
where $H_m$ is a parameter having the dimensions of the magnetic field. $m$
denotes the mean magnetisation per spin and $q$ is the number of nearest
neighbours. The mean field Hamiltonian reads
\begin{displaymath}
  \langle H_j \rangle = - g \mu_0 H_0 s_j - 2 J s_j q m 
                      = -(g \mu_0 H_0 + 2 Jq m) s_j. 
\end{displaymath}
We have reduced the original $N$--spin Hamiltonian to an effective 1 spin
Hamiltonian. Accordingly, the partition function for 1 spin reads
\begin{displaymath}
  Z_1 = \sum_{s_1=\pm1} \exp(- \beta s_1 \langle H_1 \rangle)
      = 2 \cosh \beta (q J m + H).
\end{displaymath}
We know from statistical mechanics, that the free energy per spin is given by
\begin{displaymath}
  f= - \frac{1}{\beta} \ln Z_1 = -kT \ln(2 \cosh \beta (qJm +H)).
\end{displaymath}
So that the mean magneization per spin is
\begin{equation}
\label{eq:MeanMagn}
  m = - \frac{\partial f}{\partial H} = \tanh \beta (qJm + H).
\end{equation}
Evidently the above equation is a self--consistent equation for $m$. The
solution to this equation can be found at best graphically. The correct value
of $m$ is the crossing point of the plots of the two sides of the equation.

\begin{figure}[htbp]
  \begin{center}
    
    \caption{The graphical solution of Eq. (\ref{eq:MeanMagn}).}
    \label{fig:GraphicalM}
  \end{center}
\end{figure}

Let us look at the case $H=0$. Obviously, $m=0$ is a solution and it
corresponds to a disordered paramagnetic state. However, it is more interesting
to look at the possibility of spontaneous magnetisation and to this end we set
$H=0$. . The slope of the
function $\tanh(\beta q J m)$ varies monotonically from the initial value
$\beta q J$ to zero. Since the slope of the function $m$ is unity a nonzero
solution to Eq. (\ref{eq:MeanMagn}) exists  for $\beta q J \ge 1$ or 
$kT \le qJ$. The critical temperature $T_c$ separating the $m=0$ solution from
the $m \ne 0$ solution is given by
\begin{displaymath}
  kT_c = Jq.
\end{displaymath}
For $H=0$ the magnetisation is small near $T=T_c$, and we can expand the 
$\tanh (\beta qJm)$ term in Eq. (\ref{eq:MeanMagn}) to obtain
\begin{equation}
\label{eq:MeanMagnExp}
  m= \beta q J m - \frac{1}{3} (\beta q J m)^3 + \ldots .
\end{equation}
Again we find the disordered paramagnetic state $m=0$ as a solution to Eq. 
(\ref{eq:MeanMagnExp}). The second solution is
\begin{equation}
\label{eq:MeanMagnExp2Sol}
  m = \frac{\sqrt{3}}{(\beta q J)^{3/2}} (\beta q J -1)^{1/2},
\end{equation}
corresponding to the ordered ferromagnetic state. By substituting the two
solutions for $m$ into the expression for the mean field mean energy at $H=0$
it is easy to verify, that the $m=0$ solution provides a lower free energy for
$T>T_c$ and conversely the $m\ne 0$ solution at $T<T_c$. We can now set $kT_c =
qJ$ into the expression (\ref{eq:MeanMagnExp2Sol}) and we find  for $T$ near
$T_c$ that the magnitization vanishes as
\begin{displaymath}
  m \approx \left( \frac{T_c - T}{T_c}\right)^{1/2}.
\end{displaymath}
Another quantity of interest near $T_c$ is the zero--field susceptibility (per
spin) $\chi$
\begin{displaymath}
  \chi = \lim_{H \rightarrow 0} \frac{\partial m}{\partial H}. 
\end{displaymath}
With the help of Eq. (\ref{eq:MeanMagn}) we obtain
\begin{displaymath}
  \chi = \frac{\beta(1- \tanh^2 \beta q J m)}
              {1 - \beta q J (1- \tanh^2(\beta q J m))}.
\end{displaymath}
For $T$ near $T_c$ we find
\begin{displaymath}
  \chi \approx \frac{1}{T-T_c},
\end{displaymath}
which is the famous Curie--Weiss law.

Let us consider shortly the case $H \ne 0$. Expanding Eq. (\ref{eq:MeanMagn})
to third order in $H$ with $\beta = \beta_c = 1/(qJ)$ the magnetisation at
$T_c$ is found to be
\begin{displaymath}
  m = m + \beta_c H - \frac{1}{3} (m+ \beta_c H)^3 + \ldots .
\end{displaymath}
If $m$ and $H$ are so small that 
\begin{displaymath}
  \beta_c H \ll m
\end{displaymath}
then
\begin{displaymath}
 0 = beta_c H - \frac{1}{3}m^3
\end{displaymath}
and hence
\begin{displaymath}
  m = (3 \beta_c H)^{1/3}.
\end{displaymath}

Further quantities of interest are the mean energy and the heat capacity. The
mean energy per spin is simple the average value of the interaction energy
\begin{equation}
\label{eq:IsMeanEnergy}
  \langle e \rangle = - \frac{1}{2} q J m^2.
\end{equation}
For $T >T_c$ we know that the magnetisation vanishes, $m=0$, and hence the
mean energy as well as the heat capacity vanish for all temperatures $T>T_c$.
For $T>T_c$ the mean energy is found by inserting (\ref{eq:MeanMagnExp}) 
into (\ref{eq:IsMeanEnergy})
\begin{displaymath}
  \langle e \rangle = - \frac{1}{2} qJ [\tanh(\beta(qJm+H))]^2.
\end{displaymath}
The specific heat $C$ for $T<T_c$ is now easily evaluated with the help of the
equation
\begin{displaymath}
  C = \frac{\partial \langle e \rangle}{\partial T}.
\end{displaymath}
For $T \rightarrow T_c$ from below $C \rightarrow 3k/2$. Hence mean field
theory predicts a jump in the specific heat at $T=T_c$.

Remarkably mean field theory predicts a critical point. The critical point is
characterized by an order parameter. Here it is the
magnetisation. Unfortunately, the theory is too simple. For example it
predicts a critical temperature which does not depend upon the dimension of
the system but only on the  number of nearest neighbours. The prediction of a
phase transition for the 1d Ising model is qualitatively incorrect. The mean
field prediction for $T_c$ in a 2d square lattice is $kT_c/(Jmq) = 4$ whereas
the exact result is
\begin{displaymath}
  \frac{kT_c}{J} = \frac{2}{\ln (1+ \sqrt{2})} \approx 2.269.
\end{displaymath}
However, mean field theory predicts correctly that many physical quantities
exhibit a power law behaviour near the critical point
\begin{eqnarray*}
  m(T) &\approx&  (T_c -T)^{\beta} \;\;\; (T< T_c) \\
 \chi(T) & \approx & |T- T_c|^{- \gamma} \\
m(T=T_c) & = & \approx H^{1/\delta}.
\end{eqnarray*}
The quantities $\beta$, $\gamma$ and $\delta$ are called critical 
exponents \index{critical exponents}. A comparison of the mean field exponents
with the analytical values in 2 dimensions and simulation results in 3
dimensions are found in table \ref{tab:CriticalExponents}.

\begin{table}[htbp]
  \begin{center}
  \begin{tabular}{lcclc}\hline \hline
  Quantity               & Exponent & d=2 (exact) & d=3 (sim) & MFT \\ \hline
specific heat            & $\alpha$ & 0 (log)     & 0.113     & 0 (jump) \\
order parameter          & $\beta$  & 1/8         & 0.324     & 1/2      \\
susceptibility           & $\gamma$ & 7/4         & 1.238     & 1        \\
$M\approx H^{-1/\delta}$ & $\delta$ & 15          & 4.82      & 3        \\
Correlation length       & $\nu$    & 1           & 0.629(4)  & 1/2      \\
$c(r)$ at $T=T_c$        & $\eta$   & 1/4         & 0.031(5)  & 0    \\
\hline \hline
  \end{tabular}
    \caption{Comparison of the critical exponents for the 2 and 3 
         dimensional Ising model with mean fied theory.}
    \label{tab:CriticalExponents}
  \end{center}
\end{table}

Additional information about the phase transition can be obtained from the
behaviour of the spin correlation function $c_{ij}$
\begin{displaymath}
  c_{ij} = \langle s_i s_j \rangle - m^2,
\end{displaymath}
where we have assumed that $\langle s_i \rangle =\langle s_j \rangle =m$.
It can be shown, that the spin correlation function is related to the
susceptibility 
\begin{eqnarray*}
  \chi &=& \frac{1}{NkT} \sum_{i,j=1}^N (\langle s_i s_j \rangle -
                        \langle s_i  \rangle   \langle s_j \rangle ) \\
       & = &  \frac{1}{NkT} \sum_{i,j=1}^N c_{ij} \\
       & = & \frac{1}{kT} \sum_{j=2}^N c_{1j},
\end{eqnarray*}
since all lattice sites are equivalent. Near $T_c$ the susceptibility $\chi$
diverges, and hence the number of correlated spins must increase as 
$(T_c -T)/T_c \rightarrow 0$.  In fact the neighbourhood of the critical point
is characterized by long--range correölations. To characterize these
correlations we write $c_{ij}$ as
\begin{displaymath}
  c(r) = \frac{\exp(-r/\xi)}{r^{d-2+\eta}}.
\end{displaymath}
$r$ is the distance between the two spins. $\xi$ is called the correlation
length and increases as $T \rightarrow T_c$, as
\begin{displaymath}
  \xi \approx \left( \frac{T - T_c}{T_c} \right)^{\nu}.
\end{displaymath}
At $T=T_c$ $\xi$ is infinite and $c(r)$ decays as a power law characterized by
the critical exponent $\nu$
\begin{displaymath}
  c(r) \approx \frac{1}{r^{d-2+ \eta}}.
\end{displaymath}
As a last comment let us remark that the critical exponents $\alpha$, $\beta$,
$\gamma$, $\delta$, and $\eta$ are not independent, but obey some scaling
relations. 
\cite{GouldSpornickTobochnik}

\section{The Monte Carlo Simulation}
In the preceeding section we have convinced ourselves of the necessity of an
efficient numerical algorithm for the Ising model. We will now apply the
M(RT)$^2$ algorithm to the Ising model in order to compute numerically the
expectation values we are interested in, e.g.,  the mean energy and th mean
magnetisation . The necessity of a good algorithm based on the idea of
importance sampling is made evident by the huge number of possible
configurations. Consider for example the  the 2 dimensional Ising model on a
$50 \times 50$ square lattice. The number of terms to be summed up in order to
obtain  the state function is $2^{50\times 50} = 2^{2500} \approx 10^{753}$! 
It is therefore impossible to sum up equally distributed random
configurations.

Of course, we will have to sample configurations which are distributed according
to the Boltzmann distribution, i.e.,
\begin{displaymath}
  P_{eq}[\{\sigma_i\}] \approx \exp(- \beta H[\{\sigma_i\}]).
\end{displaymath}
In order to extend the idea of the random walk in the space of spin
configurations we have to construct the conditional transition probability 
$W(\{\sigma_i'\}|\{\sigma_i \})$ according to the prescription of the 
M(RT)$^2$ algorithm
\begin{displaymath}
\stackrel{W}{\longrightarrow} \{\sigma_i\} 
\stackrel{W}{\longrightarrow} \{\sigma_i' \} 
\stackrel{W}{\longrightarrow} \{\sigma_i'' \} 
\stackrel{W}{\longrightarrow} \cdots.   
\end{displaymath}
Of course, $W$ will have to satisfy the detailed balance condition
\begin{displaymath}
  W(\{\sigma_i'\}|\{\sigma_i \}) P_{eq}[\{\sigma_i \} ] =
 W(\{\sigma_i\}|\{\sigma_i' \}) P_{eq}[\{\sigma_i' \} ].
\end{displaymath}
The Markov chain, i.e., the random walk. is realized through local updates of
single spins. Formally, we can write for the transition probability for
flipping one spin and thus for "jumping" from the configuration
$X= ( \ldots, s_{\alpha}, \ldots)$ to the next configuration
$X'= ( \ldots, -s_{\alpha}, \ldots)$
\begin{displaymath}
  T(\{ \sigma_i |\{ \sigma_i' \}) = 
        \sum_{i,j=1}^N f_{ij} \delta (\sigma_{ij}' + \sigma_{ij}),
\end{displaymath}
where $\delta(\sigma_{ij}' + \sigma_{ij})$ describes the flipping of spin
$\sigma_{ij}$: $\sigma_{ij}' \longrightarrow -\sigma_{ij}$. Of course $f_ij$
is constant and $f_{ij}=1/N^2$. Since $T$ is essentially constant
$T(X|Y) = T(Y|X)$ and the quantity $q(X|Y)$, which as w  remember enters the
expression for the acceptance of the move is simply given by the ratios of the
$P_{eq}$
\begin{displaymath}
 q(X'|X) = \frac{P_{eq}[X]}{P_{eq}[x']}.
\end{displaymath}
Denoting by $E(X)$ and by $E(X')$ the  the energy of the configuration before
and after the spin flip we have
\begin{displaymath}
  q(X'|X) = \exp(- \beta (E(X) - E(X'))).
\end{displaymath}
The new configuration can be accepted with probability
\begin{eqnarray*}
  A(X'|X) &=& \min (1, q(X'|X)) \\
          & = & \left\{
                  \begin{array}{ll}
                   \exp(- \beta (E(X) - E(X'))); & \;\;\; E(X') > E(X) \\
                    1;                           & \;\;\; E(X') < E(X). 
                   \end{array} 
                \right.
\end{eqnarray*}
The simplicity of the M(RT)$^2$ algorithm is made evident by formulating it
locally. It is clear from the expression of $q$ that we need
only know the energy difference $E(X') - E(X)$. Assume that the spin
$\sigma_{\nu}$  has been flipped $\sigma_{\nu} \longrightarrow -\sigma_{\nu}$
in changing the configuration from $X$ to $X'$. The index $\nu$ denotes a pair
of indices $i,j$. So we can write for the energy difference
\begin{displaymath}
  \Delta E_{\nu} = E(-s_{\nu}) - E(+s_{\nu}),
\end{displaymath}
where all other $(n-1)$spins remain unchanged. We consider first the case of
vanishing external magnetic field $(H=0)$. We have
\begin{eqnarray*}
E(+s_{\nu}) &=& - J s_{\nu} u_{\nu} + \cdots \\
E(-s_{\nu}) &=& +  J s_{\nu} u_{\nu} + \cdots,
\end{eqnarray*}
where 
\begin{displaymath}
  u_{\nu} \equiv \sum_{\textrm{nn}(\nu)} s_{nu}.
\end{displaymath}
The terms indicated by $\cdots$ do not contain $s_{\nu}$ and are equal. Thus
the energy difference can be written as
\begin{displaymath}
  \Delta E_{\nu} =  E(-s_{\nu}) - E(+s_{\nu})
                 = 2 J s_{\nu} u_{\nu}.
\end{displaymath}
Hence we can write $q$ as
\begin{displaymath}
  q(-s_{\nu}|s_{\nu}) = \exp(- \beta J s_{\nu} u_{\nu}).
\end{displaymath}
IN two dimensions we have of course 4 nearest neighbours and $U_{\nu}$ can
assume only the values $0$, $\pm2$, $\pm 4$. Hence $s_{\nu}u_{\nu}$ and
consequently $q$ can assume only 10 different values, which, in principle,
can be computed in advance.

Summarising the M(RT)$^2$ algorithm schematically reads:

1. Choose a spin $s_{\nu}$.

2. Calculate $u_{\nu}$, $\Delta E_{\nu}= 2J s_{\nu} u_{\nu}$, and
   $\eta \equiv \beta \Delta E_{\nu}$.

3. If $\eta < 0$, then flip the spin $s_{\nu} \rightarrow - s_{\nu}$;
   go back to 1..

4. If $\eta >0$, choose a random number $r$ equally distributed in $[0,1)$.
   If $r < \exp(-\eta)$ flip the spin $s_{\nu} \rightarrow - s_{\nu}$, else do
   not flip the spin; go back to 1. 

We have seen that $A$ can also be chosen to be
\begin{displaymath}
  A(Y|X) = \frac{q(Y|X)}{1 + q(Y|X)}.
\end{displaymath}
For the Ising model this means
\begin{eqnarray*}
  A(Y|X) &=& \frac{\exp(- \beta \Delta E)}{1 +\exp(- \beta \Delta E) } \\
         & = & \left(1 + \exp( \beta \Delta E) \right)^{-1} \\
         & = & \frac{1}{2} [1 - s_{\nu} \tanh(\beta J u_{\nu})].
\end{eqnarray*}
This is the so--called Glauber version of the M(RT)$^2$ algorithm, which
explicitly reads:

1. Choose a spin $s_{\nu}$.

2. Calculate $u_{\nu}$, $\Delta E_{\nu}= 2J s_{\nu} u_{\nu}$, and
   $\eta \equiv \beta \Delta E_{\nu}$.

3. Choose a random number $r$ equally distributed in $[0,1)$. If
\begin{displaymath}
  r < \frac{1}{1 + \exp(\eta)}
\end{displaymath}
perform the flip  $s_{\nu} \rightarrow - s_{\nu}$, otherwise $s_{\nu}$ is
unchanged; go back to 1.

In principle, the choice of the spin $s_{\nu}$ in step 1. of both versions of
the algorithm should occur at random. In practice, in order to save CPU time,
one sweeps across the lattice.


\subsection{The Code}

Listing und Beschreibung.

\begin{figure}[htbp]
  \begin{center}
    
    \caption{Configurations of the two dimensional Ising model on a 100 $\times$ 100 lattice at $\beta/\beta_c = 0.5, 0.7, 0.9, 0.95, 0.98$. Notice the growth of correlations from hight temperatures to the critical region.}
    \label{fig:IsingConfigurations}
  \end{center}
\end{figure}

\begin{figure}[htbp]
  \begin{center}
    
    \caption{Magnetisation as a function of the reduced temperature $kT/2J$ for $L=40$, $L=50$ and $L=100$.}
    
  \end{center}
\end{figure}

\begin{figure}[htbp]
  \begin{center}
    
    \caption{Magnetic susceptibility.}
  \end{center}
\end{figure}


\begin{figure}[htbp]
  \begin{center}
    
    \caption{The energy as a function of T.}
    \label{fig:IsingEnergy}
  \end{center}
\end{figure}


\begin{figure}[htbp]
  \begin{center}
    
    \caption{The specific heat as a function of T.}
    
  \end{center}
\end{figure}


\section{Data Analysis}
This section is devoted to the analysis of the data generated with the help of
the Monte Carlo simulation of the Ising model. The thermal quantities of
interest are, as we know, the mea energy $<E>$ and the heat capacity C.
It is clear that we will calculate the mean energy in the simulation as the
average of the energy of each configuration of the lattice, i.e.,
\begin{displaymath}
  < E> = <  - J \sum_{\textrm{nn}(ij)} s_i s_j - H_0 \sum_i s_i >.
\end{displaymath}
A straightforward way to calculate the heat capacity at constant external
magnetic field is from its definition $C=\partial <E>/ \partial T$. However,
it is better to determine $C$ from the statistical fluctuations of the energy
in the canonical ensemble. To recall this formula we write for the heat
capacity at constant volume
\begin{displaymath}
  C_V = \frac{\partial <E>}{\partial T} = 
       - \frac{1}{k T^2} \frac{\partial <E>}{\partial \beta}.
\end{displaymath}
From the definition of $<E>$
\begin{displaymath}
<E> = \frac{1}{Z} \sum_{s=1}^M E_s \exp(-\beta E_s),
\end{displaymath}
where the index $s$ runs over all $M$ accessible micro states of the system,
we have
\begin{displaymath}
  <E> = - \frac{\partial}{\partial \beta} \ln Z.
\end{displaymath}
Hence, we find
\begin{displaymath}
  \frac{\partial <E>}{\partial \beta} = 
   - \frac{1}{Z^2} \frac{\partial Z}{\partial \beta}
      \sum_s E_s \exp(- \beta E_s) 
    - \frac{1}{Z} 
      \sum_s E_s^2 \exp(- \beta E_s), 
\end{displaymath}
which finally can be written in the desired form for the specific heat
\begin{displaymath}
  C =   \frac{1}{kT^2}  \left(<E^2> -  <E>^2  \right).
\end{displaymath}

Other quantities of interest are the mean magnetisation $<M>$ and the zero
field magnetic susceptibility $\chi$.
 The former is calculated from its
definition 
\begin{displaymath}
  M = \sum_{i=1}{N} s_i
\end{displaymath}
and the latter from the fluctuations of the magnetisation
\begin{displaymath}
  \chi = \frac{1}{kT} \left( <M^2> - <M>^2  \right)
\end{displaymath}

\subsection{Estimation of Errors}
The Monte Carlo as well as the Molecular Dynamics simulation 
of physical systems produce raw data, e.g. the energy or the magnetisation of
the system, in form of a finite time series of correlated data. Typically,
stationary states, i.e. the equilibrium state, are investigated and the first
step in the data analysis is the estimation of the mean values which are
computed as time averages. Since the times is finite the mean values do
fluctuate: they are random variables as well. Performing the simulation another
time will lead to different estimate. So the second step of the data analysis
is the estimation of the variance of finite time averages.
We now discuss in some detail the analysis of the data
\cite{FlyvbjergPetersen,Flyvbjerg}.

\subsubsection{Preliminary Considerations}
Let us consider a Monte Carlo simulation in which some quantity, say $x$ is
computed. The simulation is performed over $n$ steps and $x_1$, $x_2$,
$\ldots$,  $x_n$ denote the result of the $n$ consecutive measurements of the
fluctuating quantity $x$. In order to be precise we denote by 
$\langle \ldots \rangle$ the expectation value with respect to the exact, in
general unknown, probability distribution $p(x)$
\begin{displaymath}
  \langle x \rangle = \int dx x p(x). 
\end{displaymath}
By $\bar{\ldots}$ we denote the average over the set 
$\{ x_1, x_2, \ldots, x_n$,
\begin{displaymath}
  \bar{x} \equiv \frac{1}{n} \sum_{i=1}^n x_i,
\end{displaymath}
which is the quantity we compute in practice. As is usual we assume ergodicity
\index{ergodicity}: the ensemble average $\langle x \rangle$ is equal to the
"time" average $\lim_{n \rightarrow \infty} \bar{x}$. As we know already,
we estimate the expectation value $\mu \equiv \langle x \rangle$ by the
average value
\begin{equation}
\label{eq:IsingMean}
  m \equiv \bar{x} = \frac{1}{n} \sum_{i=1}^n x_i.
\end{equation}
What we now need is the estimator for the variance of $m$,
\begin{equation}
\label{eq:IsingVariance}
  \sigma^2(m) = \langle m^2 \rangle - \langle m \rangle^2.
\end{equation}
Inserting, Eq. (\ref{eq:IsingMean}) into Eq. (\ref{eq:IsingVariance}) we find
that
\begin{eqnarray}
\label{eq:IsingVariance2}
   \sigma^2(m) & = & \frac{1}{n^2} \sum_{i,j}^n 
           \left( \langle x_i x_j \rangle  - \langle x_i \rangle  
                                            \langle x_j \rangle   \right)
                    \nonumber             \\
               & = &  \frac{1}{n^2} \sum_{i,j}^n \gamma_{i,j}
\end{eqnarray}
where we have introduced the correlation function 
$\gamma_{i,j}=
\langle x_i x_j \rangle - \langle x_i \rangle \langle x_j \rangle$ .
Using the invariance of the correlation function under time translations we
define 
\begin{displaymath}
  \gamma_t \equiv \gamma_{i,j}; \;\;\; t = |i-j|
\end{displaymath}
and rewrite Eq. (\ref{eq:IsingVariance2}) in the form
\begin{equation}
\label{eq:IsingVariance3}
  \sigma^2(m) = \frac{1}{n} \left[ \gamma_0 +
                        2 \sum_{t=1}^{n-1} \left(1 - \frac{t}{n}  \right) 
                                        \gamma_t \right].
\end{equation}
The above equation is often used as an estimate for $\sigma^2(m)$ inserting an
estimate for $\gamma_t$ at the appropriate place. Doing so requires some care
since the most obvious estimator $c_t$ for $\gamma_t$ 
\begin{equation}
\label{eq:IsingCt1}
  c_t = \frac{1}{n-t} \sum_{k=1}^{n-t} (x_k - \bar{x}) (x_{k+t} - \bar{x})
\end{equation}
is a biased estimator. It is easy to check that the expectation value of $c_t$
is not $\gamma_t$, but
\begin{equation}
\label{eq:IsingCt2}
  \langle c_t \rangle = \gamma_t  - \sigma^2(m) + \Delta_t,
\end{equation}
where $\Delta_t$ depends on the correlation functions $\gamma_{i,j}$. 
Although it is possible to construct with the help of
some approximations of the above expression unbiased estimators based on 
the correlation functions we prefer to describe here another way for the
estimation of $\sigma^2(m)$.

\subsubsection{The "Blocking" Method}
The method we want to describe now is to be preferred from a computational
point of view and because it gives information about the quality of the
estimate of $\sigma^2(m)$. The method is based on the repeated "blocking" of
data and works in the following way.

The data $x_1, \ldots, x_n$ are transformed into a new data set 
$x_1', \ldots, x_n'$ which is half as large
\begin{eqnarray}
  x_i' & = & \frac{1}{2} (x_{2i -1} + x_{2i}), \label{eq:IsingBlock1} \\
   n' & = & \frac{n}{2}. \label{eq:IsingBlock2}
\end{eqnarray}
For the new "blocked" data set we define a new average value as
\begin{displaymath}
  m' = \bar{x'} = \sum_{i=1}^{n'} x_i'.
\end{displaymath}
Obviously, the mean averages $m$ and $m'$ for the original and for the
"blocked"  data sets are equal
\begin{displaymath}
  m' = m.
\end{displaymath}
Accordingly, we define "blocked" correlation functions $\gamma'_{i,j}$ and
$\gamma_t'$ for the new primed data set 
\begin{displaymath}
  \gamma'_{i,j}=
\langle x'_i x'_j \rangle - \langle x'_i \rangle \langle x'_j \rangle.
\end{displaymath}
It is easy to check that $\sigma^2(m)$ is invariant under the blocking
transformation. To this end we calculate first
\begin{equation}
\label{eq:IsingGammaPrime}
  \gamma'_t = \left\{ 
                 \begin{array}{ll}
                  \gamma_0/2 + \gamma_1/2 & \textrm{for} \;\;\; t=0. \\
                   \frac{1}{4} \gamma_{2t-1} + \frac{1}{2} \gamma_{2t} 
                     + \frac{1}{4} \gamma_{2t+1} & \textrm{for} \;\;\; t>0.
                 \end{array} 
               \right. 
\end{equation}
and then
\begin{displaymath}
  \sigma^2(m') = \frac{1}{{n'}^2}  \sum_{i,j=1}{n'} {\gamma'}_{i,j} =
      \sigma^2(m).
\end{displaymath}
The invariance of the blocking transformation implies that no information is 
lost in considering the blocked data set. From Eq. (\ref{eq:IsingVariance3}) 
we know that
\begin{displaymath}
  \sigma^2(m) \ge \frac{\gamma_0}{n}
\end{displaymath}
and from (\ref{eq:IsingBlock2}) and (\ref{eq:IsingGammaPrime}) 
it follows that $\gamma_0/n$ increases every time the
"blocking" transformation is applied, unless $\gamma_1=0$. In the
latter case $\gamma_0/n$ is invariant.
The idea of the blocking method is very simple. If the computed
$\sigma^2(m')$ is not the same as the original $\sigma(m)$ we have to apply
again the blocking transformation to the data set
until the estimated $\sigma^2(m')$  is approximately the same as that 
calculated from the previous data set.

It is easy to show that 
\begin{displaymath}
  (\frac{\gamma_t}{n})_{t=0,1,2,\dots} \approx (\delta_{t,0})_{t=0,1,2,\dots}
\end{displaymath}
is a fixed point of the linear blocking transformation. 
At the fixed point  $\gamma_t =0$ for $t>0$ and hence 
\begin{displaymath}
\sigma^2(m)= \gamma_0/n.
\end{displaymath} 
In order to estimate 
$\sigma^2(m)$ we have to replace $\gamma_0$ in the above expression by its
estimate  $c_0$ which is defined in (\ref{eq:IsingCt2}). Because of Eq.
(\ref{eq:IsingCt2}) and of $\Delta_0=0$ solving for $\sigma 2(m)$ we obtain
\begin{displaymath}
\sigma^2(m)  \ge  \frac{\langle c_0\rangle}{n-1}.
\end{displaymath}
Of course, the identity is satisfied at the fixed point.



In practice one proceeds as follows. Starting from the original data set
$x_1, \ldots, x_n$ one estimates $\sigma^2(m)$ using $c_t$ as an estimate for 
$\gamma_t$, i.e. one calculates 
\begin{displaymath}
  \frac{c_0}{n-1}
\end{displaymath}
as an estimate for 
\begin{displaymath}
  \frac{\langle c_0\rangle}{n-1} \le \sigma^2(m).
\end{displaymath}
Then, the blocking transformation is applied to the data set and
\begin{displaymath}
\frac{c'_0}{(n-1)}
\end{displaymath}  
is computed. The process is repeated until $n'=2$. The sequence of values
obtained for $c_0/(n-1)$ will increase until it remains constant within the
fluctuations. This constant value is the estimate for $\sigma^2(m)$.

\begin{figure}[htbp]
  \begin{center}
    
    \caption{Estimates for $\sigma^2(m)$ obtained with the blocking method.}
    
  \end{center}
\end{figure}

\subsection{Finite Size Effects}
WICHTIG!!!!! Es gibt eine exakte Lsg. fuer das diskrete LxL 2D Ising modell
mit periodischen randbedingungen. Dies koennte man in den Figuren plotten.
A. E. Ferdinand and M. E. Fisher, Phys. Rev. 185 (1969) 832
WICHTIG !!!!!!!!

Another point which he have to consider in Monte Carlo simulations of phase
transitions is that the correlation length $\xi$ can not diverge near the
critical point. similarly, also the divergencies in other quantities are
rounded and shifted. This fact is illustrated in 
Fig. (\ref{fig:IsingFiniteSize}) where we plot the specific heat of the 
2d Ising model as a function of $\beta$ for various lattice sizes.


\begin{figure}[htbp]
\label{fig:IsingFiniteSize}
  \begin{center}
    
    \caption{Finite size scaling behaviour of the two dimensional 
       Ising model on $L \times L$ square lattices. (Exact solutions of
       Ferdinand and Fisher??).}
    
  \end{center}
\end{figure}

Because of the finiteness of the system near $T_c$ the role of the correlation
length $\xi$ is taken over by the linear size of the system
\begin{displaymath}
  \xi(T) \approx L \approx |T -T_c|^{- \nu}.
\end{displaymath}
Hence we can write
\begin{displaymath}
  | T- T_c| \approx L^{-1/\nu}
\end{displaymath}
and we see that near $T_c$  the usual scaling laws are replaced by
\begin{eqnarray*}
  m(T) & \approx & (T_c - T)^{\beta} \rightarrow L^{-\beta/\nu}, \\
  C(T) & \approx & | T- T_c|^{- \alpha} \rightarrow L^{\alpha/\nu}, \\
  \chi(T) & \approx & | T- T_c|^{- \gamma} \rightarrow L^{\gamma/\nu}.
\end{eqnarray*}
In order to estimate the critical exponents one usually fits the above 
finite--size scaling laws.



\section{The Cluster Algorithm}
The simulations of the Ising model have shown that as the temperature is
decreased, in the absence of an external field,  clusters of spins of both
signs tend to expand as we approach the transition. In this situation the
generation of new configuration with the help of local algorithms is a
difficult task since the system is almost trapped in a subset of phase
space. To overcome this difficulty cluster algorithms have been proposed
\cite{Swendsen}. The basic idea is to rearrange large blocks of spins instead
of individual spins. We will describe here the Swendsen--Wang algorithm and
will follow the presentation found in \cite{Schnakenberg}.

The prescription for the cluster update algorithms is motivated by an
appropriate equivalent representation of the state function of the Ising model.
As we know
\begin{displaymath}
  Z = \sum_{ \{ \sigma_i\} } \exp \left( \beta \sum_{nn(ij)} 
          \sigma_i \sigma_j \right).
\end{displaymath}
Using the fact that the product $\sigma_i \sigma_j$ of two Ising spins
can assume only the two values $\pm 1$, we find that
 \begin{displaymath}
\exp(\beta \sigma_i \sigma_j) =  \exp(\beta) [(1-p) + p 
 \delta_{\sigma_i \sigma_j} ],
\end{displaymath}
where we have introduced $p = 1- \exp(-2 \beta)$.
Thus above sum can be rewritten as \cite{Kasteleyn,Fortuin}
\begin{displaymath}
  Z  =  \sum_{ \{ \sigma_i\} } \prod_{nn(ij)} 
                \exp(\beta) [(1-p) + p \delta_{\sigma_i \sigma_j} ] .
\end{displaymath}
With the help of the trivial identity
\begin{displaymath}
  a + b = \sum_{n=0}^{1} (a \delta_{n,0} + b \delta_{n,1}
\end{displaymath}
we can write
\begin{displaymath}
  Z =  \sum_{ \{ \sigma_i\} } \sum_{n_{ij}} \prod_{nn(ij)}
      \exp(\beta) [(1-p)\delta_{n_{ij},0} + 
             p \delta_{\sigma_i \sigma_j} \delta_{n_{ij},1} ]    ,
\end{displaymath}
where we have introduced the bond variable $n_{ij}$ which can take the values 
$n_{ij}=0$ or $1$, corresponding to deleted or active bonds.
The idea of the cluster algorithm consists in identifying clusters of spins
with parallel spins  that are connected by active bonds and by updating all
the spins in cluster at one time. 

As we just mentioned each spin configuration contains connected domains
of parallel spins, i.e., nearest neighbour spins show in the same direction.
These domains play a central role in the Swendsen--Wang algorithm:

(i) The starting point is a configuration, say $K$. For each nearest neighbour
   pair within a domain of parallel spins in $K$ we establish with probability 
   \begin{displaymath}
     p_b = 1 - \exp(-2 \beta J)
   \end{displaymath}
    a \textit{bond}. A cluster is a region of the lattice where all lattice
    sites are connected by bonds. In this way the spin configuration is
    converted into a cluster configuration.

(ii) Each cluster is assigned a new spin value, $s=1$ with probability
   1/2 or $s=-1$ with probability 1/2.  

(iii) The new configuration $K'$ is obtained by reassigning the value of 
     the spins at each site in a cluster. All sites
   in a given cluster assume the same spin assigned in 2. to the given cluster.
   

It is clear that if the original configuration contains sizeable cluster the
new configuration $K'$ will be very different from it. With the help of a
local algorithm the new configuration would have been obtained only after a
very long time. We now have to check that the above algorithm leads to the
correct simulation of the equilibrium properties. 

To this end we consider the transitions between two spin configurations $K$
and $K'$ over a cluster configuration $C$. It follows from the algorithm that
if the transition $K \rightarrow K'$ is possible also the transition $K'
\rightarrow K$ is allowed, and hence
\begin{displaymath}
  K \leftrightarrow C \leftrightarrow K'.
\end{displaymath}
Since step (i) in the above algorithm and step (ii) and (iii)  
are independent we have
\begin{eqnarray}
\label{eq:ClusterWW}
  W(K'|K) &=&  W(K'|C) W(C|K), \\
 W(K|K') &=&  W(K|C) W(C|K'). \nonumber
\end{eqnarray}
The spins $s=\pm 1$ are assigned with equal probability to the clusters,
so it is obvious that we have
\begin{displaymath}
  W(K|C) = W(K'|C),
\end{displaymath}
and hence it follows from Eq. (\ref{eq:ClusterWW})
  \begin{displaymath}
    \frac{W(K'|K)}{W(K|K')} = \frac{W(C|K)}{W(C|K')}.
  \end{displaymath}
Let us now $r$ be the number of nearest neighbours pairs, which do have in $K$
parallel spins, but which in $C$ belong in different clusters. In the step 1.
$K \rightarrow C$ bond have not been established between these pairs with the
probability $1- p_b$. Correspondingly, $r'$ denotes the same number in $K'$. 
Then we have
\begin{equation}
\label{eq:ClusterWdW}
  frac{W(C|K)}{W(C|K')}  = (1-p_b)^{r-r'} = \exp[-2 \beta (r-r') J].
\end{equation}
Let $q$ denote the number of nearest neighbours pairs in $K$ having different
spins, correspondingly $q'$ in $K'$, and $c$ the number of nearest neighbour
pairs, which are separated in $C$ by cluster borders. Then we have
\begin{displaymath}
  r+q = r' + q' = c
\end{displaymath}
and hence
\begin{displaymath}
  r - r' = -(q-q').
\end{displaymath}
Thus Eq. (\ref{eq:ClusterWdW}) may be written as
\begin{displaymath}
 frac{W(C|K)}{W(C|K')}  =  \exp[2 \beta (q-q') J].
\end{displaymath}
The last expression can be rewritten in amore appropriate 
way by considering that the energies $E(K)$ and $E(K')$ of the configurations
$K$ and $K'$ can be written as 
\begin{eqnarray*}
  E(K) &=& -d L^d J + 2 q J,  \\
  E(K') &=& -d L^d J + 2 q' J
\end{eqnarray*}
and hence
\begin{displaymath}
   E(K) -  E(K') = 2 (q-q') J,
\end{displaymath}
so that finally
\begin{displaymath}
 frac{W(K'|K)}{W(K|K')} = \exp[\beta (E(K) - E(K'))]. 
\end{displaymath}
We immediately recognise again the detailed balance condition. This completes
the proof of the Cluster algorithms.

Prior to presenting the Java code of the Cluster algorithm for the 2
dimensional Ising model it may be helpful do give a more detailed 
description of the practical implementation of the three steps (i), (ii), and
(iii) introduced above. The experience with the Hoshen--Kopelman algorithm 
\index{Hoshen--Kopelman algorithm} for
percolation problems will turn out to be very useful. Again we follow Ref.
\cite{Schnakenberg}.

First let us remark that we will make use of helical boundary conditions.
The spin configuration will be represented in a linear array denoted by
$s_{\nu}$, where the index $\nu$ may assume the values 
$\nu=1, 2, \ldots, N=L^2$, where $L$ denotes  the number of spins in one
dimension. The index $\nu$ is easily obtained from the Cartesian coordinates
$i$, $j$
\begin{eqnarray*}
  \nu &=& i + j L; \;\;\; \textrm{for} \;\;\; d=2, \\
  \nu &=& i + j L + k L^2; \;\;\; \textrm{for} \;\;\; d=3.
\end{eqnarray*}
The nearest neighbours of the lattice site $\nu$ are easily determined:
in 2 dimensions the left and the right neighbour are at $\nu \pm 1$ and the
upper and the bottom neighbours are at $\nu \pm L$; in 3 dimension we have
furthermore a neighbour in front and on the back at $\nu= \pm L^2$. Counting
the lattice sites in this way we have essentially fixed the boundary 
conditions: If the lattice site $\nu$ is on the right 
border of the lattice its right
neighbour will be in the line below on the left border; and so on. 
We need only a prescription for the case that the numerical value
of $\mu = \nu \pm 1$, $\mu =\nu \pm L$, $\mu =\nu \pm L^2$ is 
negative or greater then $N=L^d$:
\begin{eqnarray*}
\mu \le 0 & : & \mu \rightarrow \mu + L^d \\
\mu  > L^d & : & \mu \rightarrow \mu - L^d
  \end{eqnarray*}
The helical
boundary conditions are depicted in Fig. \ref{fig:HelicalBoundary}.

\begin{figure}[htbp]
  \begin{center}
    
    \caption{Helical boundary conditions}
    \label{fig:HelicalBoundary}
  \end{center}
\end{figure}

The initial configuration of the spins $s_{\nu}$ will be saved in a linear
array $S(\nu)$. We will need to further arrays, $A(\nu)$ with
$1 \le \nu \le N$ and $B(c)$ with $c= 1,2, \ldots, C \le N$. 
For a given lattice site $\nu$ we have to distinguish whether a bond to a
nearest neighbour $\mu$ has been established or not. In the first case
$s_{\nu} = s_{\mu}$. In the second case, either $s_{\mu} \ne s_{\nu}$ or 
$s_{\nu} = s_{\mu}$ and the bond was not established with probability $1-p_b$. 


{\bf Sweep (i)}: for $\nu= 1, \ldots, N$

1. No bonds are established to the nearest neighbours $\mu= \nu -1$ and
$\mu = \nu -L$ of lattice site $\nu$. With lattice site $\nu$ begins a new
cluster with name $c$, i.e., we set $A(\nu) = c$. Since no bonds are
established starting from $\nu$ the cluster $c$ is unbound and we set
$B(c) =0$.

2. A bond is established to exactly one of the two neighbours $\mu= \nu -1$
   and $\mu = \nu -L$ of lattice site $\nu$. The nearest neighbour spin at
   $\mu$ belongs to cluster $c'$. Hence also cell $\nu$ is ascribed to cluster
   $c'$, i.e., $A(\nu) = c'$; the $B$ array is unchanged.

3. A bond is established to both nearest neighbours $\mu_1= \nu -1$
   and $\mu_2 = \nu -L$ of lattice site $\nu$ and cell $\mu_1$ and $\mu_2$
   belong to the same cluster $c'$. In this case we proceed as in 2. and set
   $A(\nu) = c'$.

4.  A bond is established to both nearest neighbours $\mu_1= \nu -1$
   and $\mu_2 = \nu -L$ of lattice site $\nu$ and cell $\mu_1$ and $\mu_2$
   belong to different clusters, say $A(\mu_1) = c_1$ and $A(\mu_2) = c_2$.
   We assume that $c_1 < c_2$. The clusters $c_1$ and $c_2$ have to be united.
   To this end we begin by attributing cell $\nu$ to the cluster with the
   smaller name, $A(\nu) = c_1$. Then cluster $c_2$ is attached to cluster
   $c_1$ by putting $B(c_2) = -c_1$. If cluster $c_2$ was already attached to
   another cluster $c_2'$, i.e., $B(c_2) = -c_2'$ then we have to go back and
   set $B(c_2') = -c_1$. Negative entries $B(c') = -c$ indicate that the
   cluster $c'$ has been attached to the cluster $c$. 

Because of the helical boundary conditions the following situation may
arise. A cell $\nu$ in the first row may establish bonds to a cell $\mu$ 
in the last row,
which has not yet been ascribed to any cluster. In this case we put 
$A(\nu) = A(\mu) = c$.

{\bf Sweep (ii)}: for $c= 1, \ldots, C$, where $C$ is the largest cluster name.

1. The cluster $c$ is not attached,  $B(c) =0$. Draw a equally distributed
   random number $1 \le r < 1$ and put $B(c) = +1$ if $r<1/2$, else put
   $B(c) = +2$.

2. The cluster $c$ is attached, $B(c) = - c' \neq 0$. The value $B(c) = B(c')$
   is taken over. If $c' > c$ as a consequence of further attachments,
   eventually step 1. has to be repeated for $B(c')$.


{\bf Sweep (iii)}: for $\nu= 1, \ldots, N$.

1.  A new configuration is generated with the help of the rule
     $S(\nu) = 2 B(A(\nu)) - 3$. 


     \begin{figure}[htbp]
       \begin{center}
         
         \caption{The stages of the Swendsen--Wang algorithm for a 
         $6\times 6$ array with helical boundary conditions. 
            IM WESENLICHEN DIE FIGUR AUS MACKEOWN, S.375 MIT 
               ANDEREN RANDBEDINGUNGEN.}
         \label{fig:Swendsen}
       \end{center}
     \end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{Bibliographies\SimulationBook}
\bibliography{Bibliographies\V_98,Bibliographies\simulit}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
