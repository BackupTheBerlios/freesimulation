\chapter{Molecular Dynamics}

\section{Introduction}
\subsection{Statistical Properties of Fluids}
With this chapter we begin the description of the application
of stochastic methods to the simulation of physical systems. Computer
simulations are an essential tool for the understanding of complex physical
systems which complement the more traditional theoretical and experimental
approaches. Computer simulations offer the possibility to investigate  the
structural, dynamical and thermodynamical properties of interesting systems. 
Within statistical physics essentially two different simulation approaches
have been developed: Molecular Dynamics (MD) methods, which we will consider
in this chapter  and Monte Carlo methods
(MC), which will be introduced in the next one.

The basic idea of the Molecular Dynamics approach is the following one: We
know that fluids, e.g. gases and liquids, are systems which are composed of a
large number of particles which are mutually interacting. 
Although the most
fundamental description of matter at the microscopic level is, of course,
quantum mechanical, in the framework of Molecular Dynamics it is assumed that
we can describe the motion of a single atom or molecule  by the classical
laws of motion, i.e., by Hamilton equations of motion. This assumption is
justified, if the mean de Broglie wavelength of an atom is much smaller
then the mean distance between atoms. This condition is satisfied if the
density of the fluid is sufficiently low  or if the temperature of the fluid
is sufficiently high. Thus, it is the aim of Molecular Dynamics to
understand the macroscopic thermodynamic and dynamical properties of a fluid
starting from the microscopic equations of motions of its atoms. To put it
differently: Molecular Dynamics is the direct simulation of the
equations of motion of a system composed of $N$ classical mutually 
interacting particles. The main numerical problem we have to address in this
chapter is the development of an algorithm for the numerical integration of
the equations of motion of $N$ particles.

The numerical solution of classical equations of motion looks like a
deterministic process. Why is it then relevant for us? 
The answer is very easy. Molecular Dynamics is a beautiful
example of a deterministic Markov process. The temporal evolution of the system
is of course deterministic, but the initial condition, the initial
configuration of the atoms in the fluid is random! 
The initial position as well as the initial velocities of the particles are
random. A Molecular Dynamics
simulation solves the Liouville equation, which as we know 
is the typical example of the differential Chapman--Kolmogorov equation 
of a deterministic Markov process.

\subsection{Some Historical Comments}
The Molecular Dynamics method has been proposed by Alder and Wainwright
in 1957 \cite{AlderWainwright57,AlderWainwright59}. They investigated a fluid
composed of hard spheres. In this pioneering simulation the particles moved
with constant velocity between two elastic collisions. The first simulation of
a real fluid was published in 1964 by Rahman \cite{Rahman}. He simulated
liquid Argon assuming a Lennard-Jones potential for the pair interaction
between the atoms (see below). The equations of motion were integrated using a
stepwise algorithm. It is worth mentioning a citation classic in this context,
namely the paper by Verlet \cite{Verlet67,Verlet68}, in which the 
algorithm for the  integration of the equations of motion, 
which bears his name has been proposed. Since this time Molecular Dynamics
developed to a standard tool in statistical physics. We just want to mention
some important steps. 1968 a diatomic molecular
fluid was simulated for the first time \cite{HarpBerne}. 1971 Rahman and
Shillinger reported the simulation of liquid water. In the same year Woodcock
investigated a fluid with Coulomb interactions -- melted KCl. In 1973 
Barojas, Levesque and Quentrec simulate rigid molecules. In 1977 Camman, Gelin
and Karplus apply Molecular Dynamics to the investigation of proteins. 
The Molecular Dynamics simulation we mentioned were all concerned with
equilibrium properties of fluids. 

At the beginning of the seventies
Non--Equilbrium Molecular Dynamics (NEMD) was developed, which allowed also the
investigation of the behaviour of fluids in typical non-equilibrium situations,
e.g., fluids in thermal gradients, or fluid in shear flow.

At the beginning of the eighties two important papers appeared which reported
on the possibility to include also quantum mechanical effects in the MD
simulations.

\subsection{The equations of motion}
The classical Molecular Dynamics algorithm solves the equation of motion for a
set of $N$ particles. A mechanical system with $3N$ degrees of freedom is
fully characterised by a set of $3N$ generalised coordinates $q_i$, 
$i=1, \ldots, 3N$ and of $3N$ generalised momenta $p_i$, 
$i=1, \ldots, 3N$. For a given Hamiltonian $H$ the equations of motion
read
\begin{displaymath}
  \dot{q}_i = \frac{\partial H}{\partial p_i} \;\;\; \mbox{for} 
 \;\;\; i=1, \ldots, N
\end{displaymath}
and
\begin{displaymath}
  \dot{p}_i = - \frac{\partial H}{\partial q_i} 
      \;\;\; \mbox{for} \;\;\; i=1, \ldots, N.
\end{displaymath}
In many cases the potential is conservative and Cartesian coordinates
$x_i$ and the velocities $v_i = p_i/m_i$ are used, where $m_i$ denotes the
mass of particle $i$. In these cases the Hamiltonian may be written as
\begin{displaymath}
  H(\{p_i,x_i\}) = \sum_{i=1}^N \frac{p_i^2}{2m_i} + V(\{x_i\}).
\end{displaymath}
For this special case the equations of motion read
\begin{equation}
   \label{HamiltonEqs}
  \dot{x}_i = v_i \;\;\; \mbox{and} \;\;\; m_i \dot{v}_i = F_i,
\end{equation}
where the forces are
\begin{displaymath}
  F_i = - \frac{\partial}{\partial x_i} V(\{x_i\}).
\end{displaymath}
It is clear from Eq. (\ref{HamiltonEqs}) that the calculation of the
velocities $v_i$ is not essential, since the Hamiltonian equations of motion 
(\ref{HamiltonEqs}) are  equivalent to the Newtonian ones
\begin{displaymath}
  m_i \frac{\partial^2}{\partial t^2}{x}_i = F_i(\{x_i\}).
\end{displaymath}
Having recalled the equations of motion of classical system composed of $N$
particles we have to address the two following questions which are essential
to all Molecular Dynamics simulations: How do we calculate the forces? How do
we integrate the equations of motion? These questions will be addressed in the
following two sections.


\section{Simple Models and interaction Potentials}
In the first Molecular Dynamics simulation the classical fluid of interest was
liquid Argon. In liquid argon the atoms are chemically inert and can be
assumed to be spherical. For this reason the forces between any two atoms 
depend only on the distance between the atoms. Thus, considering a system of
$N$ atoms the potential energy may be written as
\begin{displaymath}
  V =  \underbrace{\sum_i V_1(\vec{r}_i)}_{\textrm{external field}} + 
       \underbrace{\sum_{i}\sum_{j>i} V_2(\vec{r}_i,\vec{r}_j)}_{
            \textrm{Pair interaction}} +
       \underbrace{\sum_{i} \sum_{j>i} \sum_{k>j>i} V_3(\vec{r}_i,\vec{r}_j,
        \vec{r}_k)}_{\textrm{3-particle interaction}}
       + \cdots,
\end{displaymath}
where the first sum on the right--hand side represents the potential of an
external field, the second sum represents the pair interactions, the 
third sum represents the three particle interaction, and so on. Of course, the
calculation of the three particle interaction and of interaction of higher
order are very expensive from a computational point of view. Fortunately, in
praxis these contributions to the potential can be neglected. For simple
fluids, like Argon, the pairwise
interaction $V_2$ depends only on the magnitude of the separation 
$|\vec{r}_i - \vec{r}_j|$ between the atoms $i$ and $j$. For the moment we
will also assume that there is no external field acting on the particles, so
that we are left only with the pair interaction potential $V_2$.

This potential can, of course, be determined from first principles using
quantum mechanics. These calculations turn out to be very difficult and so one
relies upon phenomenological forms for the potential $V_2$. For argon and other
noble atoms with closed electronic shells, the mutual polarisation of each
atom induces an attractive interaction, which is known as the van der Waals
interaction. This interaction has the form
\begin{displaymath}
  V_{vdW} \approx -I \left( \frac{\sigma}{r}\right)^6,
\end{displaymath}
where $I$ is the ionisation potential of the atom, and $\sigma$ is a
length scale characterising the size of the atom. 

When two argon atoms approach each other so closely that their electron
shells overlap, the Pauli exclusion principle causes an effective repulsive
force. This force increases rapidly with decreasing separation between
the atoms. This effect is called \textit{core repulsion}. The most common
phenomenological form
of the potential $V_2$ is the Lennard--Jones potential
\begin{displaymath}
  V_{LJ} = 4 \epsilon \left[\left( \frac{\sigma}{r}\right)^{12}
                           - \left( \frac{\sigma}{r}\right)^6 \right]. 
\end{displaymath}
A plot of the Lennard--Jones potential can be seen in Fig. 
(\ref{Fig:LennardJones}).
It is important to notice that $V_{LJ}=0$ at $r= \sigma$ and that 
$V_{LJ}(r)$ is essentially equal zero for $r>3\sigma$. The parameter 
$\epsilon$ is the depth of the potential at the minimum. The minimum occurs at
the separation $r= 2^{1/6}\sigma$. The parameters
$\epsilon= 1.65 \times 10^{-21} J$ and $\sigma = 3.4 A$ are in good agreement
with the experimental properties of liquid argon.
 
\begin{figure}
\label{Fig:LennardJones}
\caption{Plot of the Lennard--Jones potential $V_{LJ}$. The potential is
  characterised by the length scale $\sigma$ and by the energy $\epsilon$.}
\end{figure}

In order to study some special properties of fluids it is sometimes useful to
consider less general potentials. The hard--sphere potential is defined as
\begin{displaymath}
  V_{hs}(r) = \left\{ 
                \begin{array}{ll}
                 \infty, &\hspace{1cm} (r<\sigma) \\
                 0, & \hspace{1cm} (\sigma \le r).
                \end{array}
             \right.
\end{displaymath}
The hard sphere potential is depicted in Fig. (\ref{Fig:HardSphere}).

\begin{figure}
\caption{Plot od the hard shere potential.}
\label{Fig:HardSphere}
\end{figure}

The Lennard--Jones potential may be approximated by the square well potential
\begin{displaymath}
  V_{sw}(r) = \left\{
              \begin{array}{ll}
                   \infty, & \hspace{1cm} (r < \sigma_1) \\
                    - \epsilon, & \hspace{1cm} (\sigma_1 \le r < \sigma_2) \\
                    0, & \hspace{1cm} (\sigma_2 \le r)  
               \end{array}
              \right. 
\end{displaymath}
The square well potential is depicted in Fig. (\ref{Fig:SquareWell}).

\begin{figure}
\label{Fig:SquareWell}
\caption{Plot of the square well potential}
\end{figure}

The soft sphere potential is defined as
\begin{displaymath}
  V_{ss}(r)= \epsilon \left( \frac{\sigma}{r} \right)^{\nu},
\end{displaymath}
where $\nu$ is a parameter controlling the strength of the potential. For
increasing $\nu$ the potential gets "harder". In Fig. (\ref{Fig:SoftSphere})
we plot the soft sphere potential for two values of $\nu$.

\begin{figure}
\label{Fig:SoftSphere}
\caption{Plot of the soft sphere potential for $\nu=1$ and $\nu=12$.}
\end{figure}

We mentioned in the introduction to this chapter that Molecular Dynamics
simulations may also be used to investigate fluids with Coulomb interaction.
In this case the potential reads
\begin{displaymath}
  V_{zz} (r_{ij}) = \frac{z_i z_j}{4 \pi \epsilon_0 r_{ij}},
\end{displaymath}
where $z_i$ and $z_j$ denote the charge of the ion $i$ and $j$, respectively.


\section{Algorithms for the Integration of Newton's equations of Motion}
In this section we want to get acquainted with algorithms for the numerical
integration of Newton's equations of motion. We will write them for notational
convenience as
\begin{displaymath}
  \frac{d^2}{dt^2} x(t) = f(x),
\end{displaymath}
where $x(t)$ and $f$ are vectors with $3N$ coordinates. 
Probably, you will already have heard a course on numerical methods for
physics and will therefore suppose that the right algorithms are Runge--Kutta
algorithms. As you may remember Runge--Kutta algorithms require the 
evaluation of the force several times per time step. Unfortunately,
in Molecular Dynamics simulations the evaluation of the forces is the most
time consuming step during the simulation. So, Runge--Kutta methods are not a
good choice. We will therefore have to look at algorithms which require at
most one or two evaluations of the forces per time step.

In the following we will consider finite difference methods in order to compute
the values of $x_{n+1}$ and $v_{n+1}$ at time $t_{n+1} = t_n + h$,
where $h$ denotes the time step of the integration. $\Delta t$ must be
chosen in such a way that the integration methods generates a stable
solution. For a conservative system $h$ must be chosen small enough to
ensure that the total energy is conserved within the required accuracy.

\subsection{Euler methods}
The easiest but not particularly precise choice are Euler algorithms. 
We write Newton's equation of motion in the following form
\begin{displaymath}
  \frac{d}{dt} x = v(t)
\end{displaymath}
and
\begin{displaymath}
  \frac{d}{dt} v = f(t).
\end{displaymath}
In order to derive an integration algorithm we look at 
$x_{n+1} = x(t_n +h)$ and $v_{n+1} = v(t_n +h)$ and expand in a
Taylor series around $x(t_n)$ and $v(t_n)$, respectively. Keeping terms up to
second order in $h$ we obtain the Euler method
\begin{eqnarray*}
  x_{n+1} &=&  x_n + h v_n + \frac{1}{2} h^2 f_n  \\
  v_{n+1} & = & v_n + h f_n.
\end{eqnarray*}
The Euler algorithm can be improved by expanding it to a predictor--corrector
scheme. The predictor step of the modified Euler algorithm reads
\begin{eqnarray*}
  y_{n+1} &=& x_n + h v_n + \frac{1}{2} h^2 f_n  \\
   f^{\ast} & = & \frac{1}{2} \left( f_n + f(y_{n+1}) \right)
\end{eqnarray*}
and the corresponding corrector step is
\begin{eqnarray*}
  x_{n+1} & = & x_n + h v_n + \frac{1}{2} h f_n \\
          & = & y_{n+1} + \frac{1}{4} 
           \left( f(y_{n+1}) - f_n \right) \\
    v_{n+1} & = & v_n + h f^{\ast},
\end{eqnarray*}
where $f_{n+1} = f(x_{n+1})$.
This modified Euler algorithm is more accurate then the original version, but
it implies a double evaluation of the forces. By choosing
\begin{displaymath}
  f_{n+1} = f(y_n)
\end{displaymath}
only one evaluation of the forces is necessary. This last predictor/corrector
Euler algorithm turns out to be less accurate then the modified Euler
algorithm.

\subsection{The Gear algorithm}
Some Molecular Dynamics simulations employ the Gear algorithm. The Gear
algorithm is a systematic predictor--corrector scheme. In the first predictor
step the positions and the velocities are developed in a Taylor series. Then
the accelerations, which in general are different from those predicted by the
Taylor expansion,  are calculated from the new positions. From the difference
of the two accelerations one calculates in the second corrector step corrections
to the positions and the velocities. Depending on the order of the Taylor
expansion which are considered one obtains Gear algorithms of different order.

Here, we just want to present the algorithm and refer the interested reader to
the original literature \cite{Gear}. The algorithm is based on the definition
of the vector
\begin{displaymath}
  \vec{x}_n = (x_n, h x_n', \frac{1}{2} h^2 x_n'', \frac{1}{3!} h^3 x_n'''
           , \ldots).
\end{displaymath}
With the help of this vector the predictor step is formulated as
\begin{displaymath}
  \vec{y}_{n+1} = A \vec{x}_n,
\end{displaymath}
and the corrector step reads
\begin{displaymath}
  \vec{x}_{n+1} = \vec{y}_{n+1} + \vec{a} \frac{1}{2} h^2
               \left( f(y_{n+1}) - y_{n+1}''\right).
\end{displaymath}
The coefficients of the matrix $A$ and of the vector $\vec{a}$ in the
predictor and in the corrector step have to be chosen differently for every
different order. To give an example, in the Gear algorithm of 4th order
the matrix $A$ is
\begin{displaymath}
  A = \left( \begin{array}{cccc}
                    1 & 1 & 1 & 1 \\
                    0 & 1 & 2 & 3 \\
                    0 & 0 & 1 & 3 \\
                    0 & 0 & 0 & 3
              \end{array} 
          \right)
\end{displaymath}
and the vector $\vec{a}$ is given by
\begin{displaymath}
  \vec{a} = \left( 
             \begin{array}{c}
               1/6 \\
               5/6 \\
               1 \\
               1/3
              \end{array}
             \right).
\end{displaymath}
For the Gear algorithm of 5th order $A$ and $\vec{a}$ have to be chosen as
\begin{displaymath}
A =   \left( 
             \begin{array}{ccccc}
                    1 & 1 & 1 & 1 & 1\\
                    0 & 1 & 2 & 3 & 4\\
                    0 & 0 & 1 & 3 & 6\\
                    0 & 0 & 0 & 3 & 4 \\
                    0 & 0 & 0 & 0 & 1
              \end{array}
             \right)
\end{displaymath}
and
\begin{displaymath}
  \vec{a} = \left( 
                 \begin{array}{c}
                   19/20 \\
                    3/4 \\
                    1 \\
                    1/2 \\
                    1/12
                  \end{array}
                    \right).
\end{displaymath}

\subsection{Verlet and Beeman algorithm}
\subsubsection{The Verlet algorithm}
One of the most popular algorithms for the integration of Newton's equations
of motion is the Verlet algorithm. The Verlet algorithm is less accurate then
the Gear algorithm but it is simpler and requires much less memory. 
Let us consider the Taylor expansion of $x(t_n + h)$ and of $x(t_n - h)$
\begin{eqnarray*}
  x_{n+1} & = &  x_n + h x_n' + \frac{1}{2} h^2 x_n'' 
         + \frac{1}{3!} h^3 x_n''' + \cdots \\
x_{n-1} & = &  x_n - h x_n' + \frac{1}{2} h^2 x_n'' 
         - \frac{1}{3!} h^3 x_n''' + \cdots 
\end{eqnarray*}
Adding the two above expansions we get
\begin{displaymath}
  x_{n+1} = 2 x_n - x_{n-1} + h^2 f(x_n).
\end{displaymath}
This is the Verlet algorithm. Note that velocities do not appear in the above
equation. If knowledge about the velocities is needed, i.e., for computing
the kinetic energy, they can be computed as central differences
\begin{equation}
\label{Eq:VerletV}
  v_n = (x_{n+1} - x_{n-1})/2h.
\end{equation}
Note, that the Verlet algorithm is not self--starting, since it requires the
knowledge of $x_n$ and $x_{n-1}$. If initially only $x_n$ and $v_n$ are known,
$x_{n+1}$ can be computed in the first step as
\begin{displaymath}
  x_{n+1} = x_n + h v_n + \frac{1}{2} h^2 f(x_n).
\end{displaymath}
The global errors associated with the Verlet algorithm are third order for the
positions and second--order for the velocities.

\subsubsection{The velocity form of the Verlet algorithm}
A mathematically equivalent version of the Verlet algorithm is the so called
velocity Verlet algorithm, which is given by
\begin{displaymath}
  x_{n+1} = x_n + v_n h + \frac{1}{2} f_n h^2
\end{displaymath}
and
\begin{displaymath}
  v_{n+1} = v_n + \frac{1}{2}(f_{n+1} + f_n) h.
\end{displaymath}
The velocity Verlet algorithm is self--starting and minimises the errors. The
equivalence of the two algorithms is best seen in the following way.
Adding the expressions for $x_{n+1}$ and for $x_{n-1}$ and eliminating the
velocities with the help of the corresponding expression we obtain immediately
the Verlet algorithm from the velocity Verlet algorithm. In the following we
will refer the velocity Verlet algorithm as \textit{the} Verlet algorihm. 

\subsubsection{The Beeman algorithm}
Another well--known algorithm is the Beeman algorithm, which also avoids the
round--off errors of the original Verlet algorithm. It can be written in the
following form
\begin{equation}
\label{Eq:BeemanX}
  x_{n+1} = x_n + v_n h + \frac{1}{6} (4 f_n - f_{n-1}) h^2
\end{equation}
and
\begin{equation}
\label{Eq:BeemanV}
  v_{n+1} = v_n + \frac{1}{6} (2 a_{n+1} + 5 f_n - f_{n-1})h.
\end{equation}
The equivalence of the Beeman algorithm with the Verlet algorithm can be
demonstrated in the following way. It follows from (\ref{Eq:BeemanV})
that
\begin{equation}
  \label{eq:BeemanV1}
  h v_{n-1} = x_n - x_{n-1} - \frac{1}{6} h^2(4 f_{n-1} - f_{n-2}).
\end{equation}
Inserting (\ref{eq:BeemanV1}) into Eq. (\ref{Eq:BeemanV}) for $v_n$ we obtain
\begin{equation}
  \label{eq:BeemanV2}
  v_n = x_n - x_{n-1} + \frac{1}{3} h^2 f_n + \frac{1}{6} h^2 f_{n-1}.
\end{equation}
Finally, the original Verlet algorithm is recovered by inserting the above
equation (\ref{eq:BeemanV2}) into Eq. (\ref{Eq:BeemanX}).

The velocity Verlet
algorithm is probably the most popular integrator for MD applications. It is
however important to remark that it is not necessary to use it. The only
criterion for the quality of an algorithm is, for systems without
thermalization, the conservation of energy. All the algorithms we presented
have a stable mean value of the total energy. Of course, the 
actual value of the
energy may fluctuate. As a matter of fact such deviations are less relevant in
algorithms with a higher precision, however the latter may show a drift in the
mean value of the energy.

\subsection{The comparison of the algorithms}
In order to compare the algorithms we consider the harmonic oscillator
\begin{displaymath}
  \frac{d^2}{dt^2}x = - x,
\end{displaymath}
with initial condition $x(0) = 1$ and $v(0)= 0$.
The exact solution to the above equation is of course
\begin{displaymath}
  x(t) = \cos t \;\;\; v(t) = - \sin t.
\end{displaymath}

Das gibt eine schoene Uebung!!!!!!!!
Plot Vergleich error,...


\section{The Algorithm for the Simulation}
\subsection{Periodic Boundary conditions}
We have seen in the previous section that a Molecular Dynamics code has to 
store at
least $3N$ coordinates of the $N$ molecules we are interested in. To this
minimal requirements we have to add $3N$ velocities, $N(N-1)/2$ pair energies,
$3N$ forces and so on. It is clear that depending on the computer at our
disposal the number $N$ of particles can not be made arbitrarily large. Since
the aim of a Molecular Dynamics simulation is to 
understand the properties of a bulk system, which is typically
composed of $N=10^{23} - 10^{25}$ particles, it is clear that we can
only simulate a fraction of this particles. Typically Molecular Dynamics
simulation operate with $10^3$ up to $10^5$ particles. It is clear that it
would not be clever to enclose these particle in a box. In contrast to bulk
systems the fraction of particles near the walls would be very large, and 
surface effects would have a dominant role. For example, if we consider
a cubic lattice of 512 particles, 296 particles will be on the surface of the
cube. This amounts to 58\% of the total number of the particles.

In oder to minimise surface effects and simulate more closely the properties
of a bulk system the following trick is used. One considers a cubic box. The
length of the side of the box is $L$. The box contains $N$ particles and it is
assumed to be in the bulk of the fluid. The box is understood as an
element of a lattice of identical cells, which also contain $N$ particles,
which are spatially distributed as in the original cell. A particle near the
border of the cell is surrounded not only from the particles in the original
box but also from the neighbouring particles in the surrounding boxes. If a
particle leaves the box in the course of the simulation through one side of the
cubic cell, an identical particle enters the box from the opposite side, so
that the number of particles in the box remains constant. These kind of
boundary conditions are called \textit{periodic boundary conditions}
\index{boundary conditions!periodic} they are illustrated in Fig. 
(\ref{fig:PeriodicBoundary}). Usually, only the coordinates of the original
particles or of their images in the simulation box are saved.

\begin{figure}[htbp]
  \begin{center}
    
    \caption{Periodic boundary conditions.}
    \label{fig:PeriodicBoundary}
  \end{center}
\end{figure}

In praxis we have to proceed as follows. Let \verb|x[i],y[i],z[i]| 
be the coordinates of the
particle with number \verb|i|, and let \verb|L| be the length of the side of
the simulation box.
After each change of the
position of the particles during the MD simulation we have to perform the
following operations
\begin{verbatim}
if (x[i] > L ) x[i] = x[i] - L;
if (x[i] < 0 ) x[i] = x[i] + L;
if (y[i] > L ) y[i] = y[i] - L;
if (y[i] < 0 ) y[i] = y[i] + L;
if (z[i] > L ) z[i] = z[i] - L;
if (z[i] < 0 ) z[i] = z[i] + L;
\end{verbatim}

\subsection{Potential cutoff}
\index{potential cutoff}
Because of the periodic boundary conditions we have to compute the forces in
an infinite system. Of course, this is not feasible. Consider the
Lennard--Jones potential. 
If the distance between two particles is $3 \sigma$ the potential energy is
about a half \% of the minimal value $-\epsilon$. It would imply a waste in
computational power to try to calculate this small rest. In praxis, for all
neutral systems with Lennard--Jones like potential it is safe to calculate 
pair--interaction energies only up to the so--called cutoff radius, $r_c$, and
to set the potential energy and the forces for large distances equal to zero,
i.e.,
\begin{displaymath}
  V(r) = \left\{ 
              \begin{array}{ll}
             V(r),& \;\;\; \textrm{for} \;\;\; r<r_c \\
                0, & \textrm{otherwise}.
               \end{array}
           \right.
\end{displaymath}
For the Lennard--Jones potential the cutoff radius is usually chosen to be
$r_c \approx 2.5 \sigma$.

\subsection{The minimum image convention}
\index{minimum image convention}
If the cutoff radius $r_c$ is smaller than half of the length of the
simulation box $L$, the minimum image convention may be used. 
The interaction partners of particle $i$ are those $N-1$ particles (images)
which lie within an imaginary box, which has the same size as the original
simulation box, but is centred around the position of particle $i$. Through
this convention the particles in the simulation box interact only with
particles which are either themselves in the simulation box or which are
shifted in each coordinate by a factor of $+L$ of $-L$ 
(see Fig. (\ref{fig:MinimumImage})).

\begin{figure}[htbp]
  \begin{center}
    
    \caption{The minimum image convention.}
    \label{fig:MinimumImage}
  \end{center}
\end{figure}

We consider the interaction between particle \verb|i| and particle \verb|j|.
The coordinate differences for the particles in the simulation box are denoted
by \verb|xij|, \verb|yij|, and \verb|zij|. These can be simply evaluated as
\begin{verbatim}
xij = x[i] - x[j];
yij = y[i] - y[j];
zij = z[i] - z[j];
\end{verbatim} 
The minimum image convention \index{minimum image convention} is realized
through the following code
\begin{verbatim}
if (xij > L/2 ) xij = xij - L;
if (yij < -L/2) xij = xij + L;
\end{verbatim}
and similarly for \verb|yij| and \verb|zij|. From the infinite number of
images of particle $j$ we pick out that copy of it, whose distance from 
particle $i$ may be  less than $r_c$. The minimum distance convention
pair--distance is then computed as
\begin{verbatim}
rij = Math.sqrt(xij*xij + yij*yij + zij*zij);
\end{verbatim}
It is important to remark that in the minimum image convention each particle
interacts exactly with $(N-1)$ particles.

\subsection{Reduced variables}
In a MD simulation the equations of motion of the particles are integrated
many times. It is therefore important to keep numerical roundoff errors as
small as possible. A prerequisite for small roundoff errors is to choose units
in such a way that the quantities which we want to compute are of the order of
unity. The fluids we are concerned with here are Lennard--Jones fluids. So it
is natural to assume that the  
units of distance and energy are Lennard--Jones parameters $\sigma$ and
$\epsilon$. The units of mass is typically chosen to be the mass of one atom
$m$. All other quantities can be expressed in terms of $\sigma$, $\epsilon$
and $m$. Accordingly, velocities are measured in units of 
$(\epsilon/m)^{1/2}$, and time is measured in units of  
$\sigma(m/\epsilon)^{1/2}$. All program variables are expressed in reduced
units. Table (\ref{tab:SystemUnitsMD}) summarises the 
system of units used in typical
molecular dynamics simulations  and shows the corresponding values for argon.

\begin{table}[htbp]
  \begin{center}
    \begin{tabular}{lll} \hline \hline
    quantity    & unit                 & value for argon \\ \hline
    length      & $\sigma$             &    $3.4 \times 10^{-10} m$ \\
    energy      &   $\epsilon$         & $1.65\times 10^{-21} J$ \\
    time        & $\sigma (m/\epsilon)^{1/2}$ & $2.17 \times 10^{-12}s$ \\
    velocity    & $(\epsilon/m)^{1/2}$ & $1.57 \times 10^2 m/s$ \\
    force       & $\epsilon/\sigma$    & $4.85 \times 10^{-12} N$ \\
    temperature & $\epsilon/k$         & $120 K$ \\
    pressure    & $\epsilon/\sigma^2$  & $1.43 \times 10^{-2}Nm^{-1}$ \\
    \hline \hline  
      \end{tabular}
    \caption{The system of units used in molecular dynamics simulation of
    particles interacting via a Lennard--Jones potential. The numerical values
    for $\sigma$, $\epsilon$ and $m$ are for argon. The quantity $k$ is
    Boltzmann's constant and has the value $k=1.38 \times 10^{-23} J/K$. The
    unit of pressure is for a two--dimensional system.}
    \label{tab:SystemUnitsMD}
  \end{center}
\end{table}
As an example, let us consider a typical molecular program  with a
dimensionless time
step of $\Delta t= 0.01$ which runs for 10000 steps. The total time of the run
is $10000 \times 0.01 = 100$ in reduced units or $2.17 \times 10^{-10}s$ for
argon. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\section{Advanced AWT Features}
\label{sec:AWTAdvanced}
As a short inlet, we want to discuss some advanced features of the Java
AWT package, so that you are able to understand all the code
of the upcoming simulations.

\subsection{Mouse Cursor}
If you want to use different cursors for the mouse, you have to take
a look at the java.awt.Cursor class. You can define a cursor for every
component displayed on the screen. So if you have a canvas and want to 
notify the user that there is a calculation going on in this canvas,
you just employ the commands:
\begin{verbatim}
Canvas canv;
canv.setCursor(Cursor.WAIT_CURSOR); // calculation starts
.....
canv.setCursor(Cursor.DEFAULT_CURSOR); // calculation is finished
\end{verbatim}

\subsection{ScrollPanes}
To use a ScrollPane you just instantiate one, use a Panel inside the
ScrollPane if you want to put many components in it and at last
add the ScrollPane to a Container (like a Frame). 
\lstinputlisting{Listings_Java/ScrollPaneDemo.java} 

\subsection{Properties and Resources}
Properties are like Xdefaults in a UNIX environment and deliver
information about the run-time environment. By reading the property list
a program can set defaults, choose colors and fonts and more. In Java 1.1
there are 21 system properties defined. Only 9 of them can be accessed
by applets, but an application has access to all of them.

So for example, you can get the file seperator to distinguish between
the \verb|/| on UNIX systems and the \verb|\| on Windows systems. You
could just use (both as application or applet accesible):
\begin{lstlisting}{}
  String s = System.getProperty (file.separator);
  System.out.println(s);
\end{lstlisting}
Some important properties are listed in table \ref{tab:Properties}.
The properties are usually defined in a file somewhere in the java
installation. You can also define your own properties and use them
for example to read parameters from local files in a browser. This is
the only way to do this, because of security reasons.
\begin{table}[htbp]
  \begin{center}
    \begin{tabular}{l|c|c|c}
      Name & Description & Sample Values & Access in Applet \\
      \verb|file.separator| & File separator & \verb|/| or \verb|\| & YES \\
      \verb|java.vendor| & JVM vendor & \verb|Netscape Communications| & YES \\
      \verb|line.separator| & Line separator & \verb|\n| & YES \\
      \verb|os.arch| & Operating system architecture & \verb|x86| or \verb|80486| & YES \\
      \verb|os.name| & Operating system name & \verb|Linux| or \verb|Windows NT| & YES \\
      \verb|path.separator| & Path separator & \verb|:| or \verb|;| & YES \\
      \verb|user.home| & User´s home directory & \verb|/home/john| & NO \\
      \verb|user.name| & User´s login name & \verb|john| & NO \\
   \end{tabular}
    \caption{A list of important properties to be read out by a Java program.}
    \label{tab:Properties}
  \end{center}
\end{table}

Another interesting topic are the resource bundles. With this you can
supply a file containing language depenedent information to your
Java program. This is similar to using your own properties (called
server properties). The package responsible for the functionality is
the \verb|java.util.Properties| package. For a complete description look
at the API documentation or take a look at \cite[]{JavaAWTReference}. 


\subsection{Events}


\subsection{A Complete GUI for the Molecular Dynamics Program}
putting it all together !

\subsection{Features not discussed in this book}
Not mentioned: Cut and Paste (Clipboard), Images, Sound 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\section{A molecular dynamics program}
Hier soll das Programm MolDyn eingefuehrt und erlaeutert werden.
Typisches Bild zeigen. Ablaufdiagramm. Javac mit option fuer grosse jobs!
Ansprechen: Anfangsbedingungen der Orte, Anfangsbedingungen der
Geschwindigkeiten (no drift!!); usw.

Check der Gesamtenergie, Trajektorien angucken. Sensitivity of initial
conditions: Drift in der Geschwindigkeit; alle Geschwindigkeiten gleich

Nur Listing mit dem code; kein GUI!!! 

GUI als Appendix !!!


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Analysis of the Results}
We already mentioned that the aim of a Molecular Dynamics simulation is to
investigate macroscopic properties of a fluid starting from the microscopic
dynamics of a system of particles. In this section we want to sketch some of
the basic techniques which allow the extraction of useful information from the
raw data. It is important to keep in mind that the conventional molecular
dynamics simulation we described so far keep the energy constant. In other
words, it is based upon the micro canonical ($E,V,N$) ensemble.



\subsection{The pair correlation function}
The best way to characterise the structure of a monoatomic fluid is to look at
the pair-correlation function $g(r)$. The radial distribution function $g(r)$
is a measure of the correlations of the positions of the particles, which are 
induced by the interactions. Let $n(\vec{r})$ denote the local particle
density. This quantity will vary from point to point, and it will fluctuate
around the mean value 
\begin{displaymath}
  n = \langle n(\vec{r}) \rangle = \frac{N}{V}.
\end{displaymath}
The local densities at two neighbouring points $\vec{r}_1$ and $\vec{r}_2$
will not be completely independent from each other. In fact, in the time
average they show certain correlations. The right measure for these
statistical correlations is the density--density pair correlation function
\begin{displaymath}
  G(\vec{r}_1,\vec{r}_2) = \frac{n(\vec{r}_1)n(\vec{r}_2)}{n^2}.
\end{displaymath}

The microscopic definition of the local density is, of course,
a sum of $\delta$--functions concentrated around the position of the particles
\begin{displaymath}
  n(\vec{r}) = \sum_{i=1}^N \delta(\vec{r} - \vec{r}_i),
\end{displaymath}
and the density--density correlation function is accordingly given by
\begin{displaymath}
   G(\vec{r}_1,\vec{r}_2) = \frac{1}{n^2} 
          \left\langle 
            \sum_{i=1}^N \sum_{j=1}^N \delta(\vec{r}_1 - \vec{r}_i)
                                      \delta(\vec{r}_2 - \vec{r}_j).
          \right\rangle 
\end{displaymath}
Introducing $\vec{r}_{12} = \vec{r}_2 - \vec{r}_1$ and 
$\vec{r}_{ij} = \vec{r}_j - \vec{r}_i$ the above equation can be written in
the equivalent form
\begin{displaymath}
  G(\vec{r}_1,\vec{r}_2) = G(\vec{r}_{12}) =
              \frac{1}{n} \delta(\vec{r}_{12}) +
              \frac{V}{N^2} 
               \left\langle
               \sum_{i=1}^N \sum_{j=1; j\neq i}^N 
               \delta(\vec{r}_{12} - \vec{r}_{ij}).
                \right\rangle      
\end{displaymath}
The function
\begin{displaymath}
g(\vec{r}_{12}) = \frac{V}{N^2} 
               \left\langle
               \sum_{i=1}^N \sum_{j=1; j\neq i}^N 
               \delta(\vec{r}_{12} - \vec{r}_{ij}).
                \right\rangle 
\end{displaymath}
is called the pair correlation function. For an isotropic Lennard--Jones fluid
the pair correlation function does not depend upon the direction of the vector 
$\vec{r}_{12}$, but only on its absolute value,
\begin{equation}
\label{eq:gMDdef}
g(\vec{r}_{12}) =
g(r)= \frac{V}{4 \pi r^2N^2} 
               \left\langle
               \sum_{i=1}^N \sum_{j=1; J\neq i}^N 
               \delta(r_{12} - r_{ij}).
                \right\rangle 
\end{equation}
The pair correlation function $g(r)$ has a clear physical meaning.
$n g(r)$ is nothing but the mean number of particles in a spherical shell
between $(r, r+dr)$ around a given particle. For large values of $r$ the
product $ng(r)$ converges against the mean particle density $n$, i.e. we have
\begin{displaymath}
  g(r\rightarrow \infty) = 1.
\end{displaymath}
For short distances the interaction between the particles is repulsive, so
that we must have
\begin{displaymath}
  g(r \rightarrow 0) = 0.
\end{displaymath}
The qualitative behaviour of $g(r)$ can be seen in 
Fig. (\ref{Fig:QualitativePair}). After a forbidden region for $r<\sigma$ 
which is caused by the fact that particles cannot penetrate each other, 
there is a sharp maximum which corresponds to the first shell of neighbouring
molecules. This first shell induces a second forbidden region characterised by
a minimum in $g(r)$ which in turn induces again a maximum characterising a
second shell of atoms, and so on until $g(r)$ reaches its 
asymptotic value of 1. 

\begin{figure}[htbp]
  \begin{center}
    
\caption{Qualitative behaviour of the pair correlation function $g(r)$ for 
         a Lennard--Jones fluid.}
    \label{Fig:QualitativePair}
  \end{center}
\end{figure}

In a simulation the function $g(r)$ can be determined in the following way.
$ng(r)$ denotes the probability to find given a particle at $r=0$ 
another particle in a distance $r$. This probability can be estimated from the
relative frequency, with which distances between the particles are found. For
the computation of $g(r)$ for $0< r < 3\sigma$ we divide this length 
$(3\sigma)$ into 100 parts $i=1,\ldots,100$ of equal length 
$\Delta r = 3 \sigma/100$. In the code we will denote this quantity by 
\verb|deltar|. For a given configuration of the $N$ particles we count how
many of the pair distances fall in each of the intervals. Algorithmically, we
have to calculate the distance $r_{ij}$ for each pair of particles. For each
$r_{ij}$ we determine the number of the corresponding channel in the histogram
according to
\begin{displaymath}
n = [r_{ij}/\Delta r] +1,
\end{displaymath}
where $[x]$ denotes the nearest integer number smaller than $x$. The value of
$g(n)$ in the $n$th channel can then be increased by 1, and so on.
This part of the code may look like
\begin{verbatim}
for (int i=1; i< N; i++) {
    for (int j=i+1; j < N+1; j++) {
    xx = x[i] - x[j];
    yy = y[i] - y[j];
    zz = z[i] - z[j];
    // periodic boundary conditions
    if (xx > L/2)  xx = xx - L;
    if (xx < -L/2)  xx = xx + L;
    if (yy > L/2)  yy = yy - L;
    if (yy < -L/2)  yy = yy + L;
    if (zz > L/2)  zz = zz - L;
    if (zz < -L/2)  zz = zz + L;
    r2 = xx*xx + yy*yy + zz*zz;
    r = Math.sqrt(r2);
    n= Mqath.floor(r/deltar) + 1;
    if (n < 100) g[n] = g[n] +2 
    }
}
\end{verbatim}
Note that we have always added 2 on \verb|g[n]| since we have counted pairs of
particles only once. The above procedure should not be performed at each time
step, since during one time step the configurations do not change
significantly. In order to increase the statistics it is better to use
statistically independent configurations. In principle, we should check for
the statistical independence. In practice, it is sufficient to consider a
configuration as independent after 10 -- 50 molecular dynamics steps.

Having evaluated a sufficient number of statically independent configurations 
we have to normalise the function $g(n)$. For an ideal gas with
$ng(r) = n$ we would find in the $i$--th spherical shell
between $(i-1)\Delta r$ and $i \Delta r$ around one particle
for each configuration in the mean
\begin{displaymath}
  \Delta N(i) = \frac{4(N-1)\pi}{3V}
            [i^3 - (i-1)^3] \Delta r^3
\end{displaymath}
particles.The actual number of pairs per configuration has to be divided by
this number. The corresponding code, which have to be placed at the end of the
simulation,  reads
\begin{verbatim}
for (int i=1; i< 101; i++){
    deltan = Math.Pi*(N-1)/(3*Volume)
              * (Math.pow(i,3) - Math.pow(i-1,3)*Math.pow(deltar,3);
    g[i] = g[i] / (Nconfig*deltan*(N-1)); 
}
\end{verbatim}
where \verb|Nconfig| denotes the number of configurations which have been
analysed. 

The computation of $g(n)$ can, of course, be placed in a part of the
program where the distances are computed anyway. However, since we have to
compute the function only every 10--50 steps an eventual redundant 
calculation of the distances does
not affect significantly the performance of the code.

The pair correlation function is of great theoretical and practical
importance. The function $g(r)$ is directly accessible in neutron and Roentgen
scattering experiments. From a theoretical point of view there is an exact
relation between the pair potential $U(r)$ and $g(r)$. 
This exact relation is the starting point for several theoretical
investigations. E.g., with some simplifying
assumptions  this relation makes possible the  theoretical calculation of
$g(r)$. 

For a given pair potential $U(r)$ the canonical configurational 
partition function of a $N$ particle system reads
\begin{displaymath}
  Q = \frac{1}{N!} \int_V \ldots \int_V
  \exp\left(
     - \frac{1}{2} \sum_i \sum_{j \neq i} U(r_{ij})/kT \right) 
          d\vec{r}_1 \ldots d\vec{r}_N.
\end{displaymath}
The above simply means that the probability to find a certain configuration, 
say $(\vec{r}_1, \ldots, \vec{r}_N)$ is given by
\begin{displaymath}
  \frac{1}{N!Q} 
  \exp\left(
     - \frac{1}{2} \sum_i \sum_{j \neq i} U(r_{ij})/kT \right) 
          d\vec{r}_1 \ldots d\vec{r}_N .
\end{displaymath}
Hence, the probability to find a particle at $\vec{r}_1$ and, at the same time
a particle at $\vec{r}_2$ regardless of the position of the other particles
is given by the two--particle distribution function
\begin{displaymath}
  n(\vec{r}_1, \vec{r}_2) = \frac{1}{(N-2)! Q}
      \int_V \ldots \int_V
  \exp\left(
     - \frac{1}{2} \sum_i \sum_{j \neq i} U(r_{ij})/kT \right)
          d\vec{r}_3 \ldots d\vec{r}_N.    
\end{displaymath}
It is now possible to demonstrate that the above function is related to the
pair correlation function through the equation
\begin{displaymath}
  g(r) = g(\vec{r}_1, \vec{r}_2) = \frac{1}{n^2}  n(\vec{r}_1, \vec{r}_2).
\end{displaymath}
It is this equation which is at the basis of several microscopic theories of
fluids. 

By far the most important
property of the function $g(r)$ is that it allows the determination  of
thermodynamic equilibrium quantities. The internal energy $U_i$ is 
nothing but the mean potential energy of the total system,
\begin{displaymath}
  U_i = \left\langle 
         \frac{1}{2} \sum_{i=1}^N \sum_{j=1, j \neq i}^N 
                       U(r_{ij}) 
         \right\rangle.
\end{displaymath}
Writing
\begin{displaymath}
  U(r_{ij}) = \int dr U(r) \delta(r- r_{ij}),
\end{displaymath}
we obtain the following expression for the internal energy
\begin{displaymath}
  U_i = \frac{1}{2} \int dr U(r)  
            \left\langle \sum_{i=1}^N \sum_{j=1, j \neq i}^N 
                      \delta(r- r_{ij})
            \right\rangle.        
\end{displaymath}
Inserting the definition (\ref{eq:gMDdef} ) 
of the pair correlation function $g(r)$ in the above
equation we get the energy equation \index{energy equation}
\begin{displaymath}
  U_i = \frac{N^2}{2V} \int_V d\vec{r} U(r) g(r).
\end{displaymath}
 
In a similar way we can deduce from the virial theorem \index{virial theorem}
the following equation  for the pressure $p$
\begin{displaymath}
  p = \frac{NkT}{V} - \frac{1}{6V}
            \left\langle
               \sum_{i=1}^N \sum_{j=1, j \neq i}^N 
                 \vec{r}_{ij} \cdot \vec{F}_{ij}
             \right\rangle ,
\end{displaymath}
which can be expressed with the help of the pair correlation function 
in the following form
\begin{displaymath}
p = \frac{NkT}{V} - \frac{n^2}{6} \int_V d\vec{r} r 
             \frac{\partial U(r)}{\partial r} g(r).  
\end{displaymath}

\begin{figure}[htbp]
  \begin{center}
    
    \caption{The pressure }
    \label{fig:PressureMD}
  \end{center}
\end{figure}

\subsection{Thermodynamic quantities}
From the principle of equipartition of energy it follows
that at equilibrium each degree of freedom in the system has a kinetic energy
$\frac{1}{2} kT$. Hence for $N$ particles in a 3d simulation the 
kinetic energy will be given by
\begin{displaymath}
  \left\langle \frac{m_i}{2}v_i^2  \right\rangle     =\frac{3}{2} NkT.
\end{displaymath}
The above expression is at the basis of the evaluation of the temperature,
since the quantity between angular brackets is easily evaluated as a time
average. 

\begin{figure}[htbp]
  \begin{center}
    
    \caption{The temperature as a function of time in Molecular Dynamic 
             simulation of a micro-canonical ensemble.}
    \label{fig:TempMicroCanMD}
  \end{center}
\end{figure}

Hier fehlen noch einige Bemerkungen.

Def. von Pot. Energie, usw. Diesen Abschnitt vor g(r)!!!!!!

\subsection{Dynamical quantities}
Up to now we have considered only static equilibrium properties of the fluid.
Time--dependent correlation functions are important tools for the description
of dynamical phenomena. In particular we are interested in understanding the
transport of particles from the equilibrium properties of the fluid. 
A typical example is the calculation of the diffusion
coefficient from the one particle velocity autocorrelation function $C(\tau)$
\begin{displaymath}
  C(\tau) = \frac{\langle \vec{v}(\tau) \vec{v}(0) \rangle}{v^2},
\end{displaymath}
or from the square of the displacement 
$\Delta(t) = \vec{r}(t) - \vec{r}(0)$. As we will
see shortly the two methods are equivalent. 

In a molecular dynamics simulation the velocity autocorrelation function is
computed as a histogram of the function $C(\tau)$ from the velocities of the
particles $\vec{v}_i(t_k)$:
\begin{displaymath}
  C(n \Delta t) =  \frac{1}{N<v^2>} \frac{1}{S} \sum_{s=1}^S \sum_{i=1}^N
                     \vec{v}_i(t_s) \cdot \vec{v}_i(t_s +n\Delta t).
\end{displaymath}
Of course, the time $t_s$ have to be chosen such that only statistical
independent pieces of the trajectories are evaluated. Usually it is sufficient
to choose $\Delta t-s =50 \Delta t$.

An autocorrelation function for a Lennard-Jones fluid can be seen in 
Fig. (\ref{fig:AutoCorrMD}).

\begin{figure}[htbp]
  \begin{center}
    
    \caption{Plot of the velocity autocorrelation function of a 
            Lennard--Jones fluid.}
    \label{fig:AutoCorrMD}
  \end{center}
\end{figure}

The figure makes evident that the particles forget rather rapidly their
starting velocity. As a consequence of the interaction with the
neighbouring atoms the magnitude and the direction of the velocity changes
rapidly and the product $\vec{v}(0) \vec{v}(\tau)$ decreases to zero. Since
each particle is surrounded by a shell of neighbours after some time there is
a reversal of the direction of motion. This explains the negative part of the
autocorrelation function. For higher temperature or lower densities this
effect does not occur. For larger time the memory of the original motions 
completely lost and the correlation function tends definitely to zero.

As we mentioned at the beginning of this subsection the velocity
autocorrelation function is deeply related to the diffusive Brownian motion of
the atoms in the fluid. In particular we have the  simple relation between
the velocity autocorrelation function and the displacement
\begin{equation}
\label{eq:DisplAutoCorrMD}
  \frac{d^2}{dt^2} \langle \Delta(\tau)^2 \rangle =
      2 \langle \vec{v}(0) \vec{v}(\tau) \rangle
\end{equation}
The above relation is easily demonstrated by looking at
\begin{eqnarray*}
  \frac{d}{dt}  \Delta(\tau)^2  
          & = &     \frac{d}{dt} [\vec{r}(t) - \vec{r}(0)]^2 \\
          & = & 2 \vec{v} \cdot [\vec{r}(t) - \vec{r}(0) ]  \\
          & = & 2 \int_0^t d\tau \vec{v}(t) \cdot \vec{v}(\tau).
\end{eqnarray*}
Because of the symmetry of $\langle \vec{v}(0) \vec{v}(\tau) \rangle$ 
for arbitrary times $t$ we conclude that
\begin{displaymath}
  \frac{d}{dt}  \Delta(\tau)^2 = 2 \int_0^t d\tau \vec{v}(\tau) 
          \cdot \vec{v}(0).
\end{displaymath}
The statement (\ref{eq:DisplAutoCorrMD}) follows immediately.

We know also that the displacement $\langle \Delta^2 \rangle$ 
is related to the diffusion
constant by the relation 
\begin{displaymath}
  \lim_{\tau \rightarrow \infty}\langle \Delta^2(\tau) \rangle / \tau = 6D.
\end{displaymath}
A very important relation follows now
\begin{displaymath}
  D = \frac{1}{3} \int_0^{\infty}  d\tau
            \langle \vec{v}(0) \vec{v}(\tau) \rangle .
\end{displaymath}
The above equation is a typical example of a Kubo relation. Similar relations
hold also for other thermodynamic transport coefficients.


\section{Molecular Dynamics at Constant Temperature}
Up to now the equilbrum properties of the system were fixed by the volume of
the simulation cell, by the initial position of the particles and by the
initial velocities. In such a micro canonical ensemble ($NVE$) it is easy to
determine the constant energy as the sum of the potential and the kinetic
energy. As we have seen in the previous runs of the program \verb|MolDyn| the
temperature and the pressure of the fluid fluctuate around some mean values,
which can be computed as time averages. In many thermodynamic applications
we are interested in having an ensemble at constant temperature, i.e. a
canonical ensemble ($NVT$). It is therefore the subject of the section to
derive a canonical molecular dynamics simulation algorithm.

\subsection{Velocity rescaling}
The easiest way to keep the temperature fixed to a wished temperature
is to rescale the velocities at each time step. Let us be more precise. We
denote by $T_0$ the desired temperature and by $K_0$ the corresponding kinetic
energy. By $T$ and $K$ we denoted the actual temperature, respectively kinetic
energy of the $N$--particle system. After each time step we rescale 
the velocities according to the prescription
\begin{displaymath}
  \vec{v}_i \longrightarrow \left( \frac{T_0}{T} \right)^{1/2} \vec{v}_i
          = \left( \frac{K_0}{K} \right)^{1/2} \vec{v}_i.
\end{displaymath}
To put it differently, after each time step $\delta t$ we correct the velocity
by an amount $\delta \vec{v}_i$
\begin{displaymath}
  \vec{v}_i \longrightarrow \vec{v}_i + \delta \vec{v}_i
\end{displaymath}
where
\begin{displaymath}
  \delta \vec{v}_i = \left[ \left( 
                  \frac{K_0}{K}\right)^{1/2} -1  
                       \right]\vec{v}_i 
                    = \left[ \left(
                        \frac{K_0}{K_0 + \delta K}\right)^{1/2} -1 
                       \right]\vec{v}_i.  
\end{displaymath}
In the above expression we have introduced the change in the kinetic energy 
$\delta K = K- K_0$, $|\delta K| \ll 1$. Expanding the terms on the squared
brackets up to terms of first order in $\delta K$ we obtain
\begin{displaymath}
  \delta \vec{v}_i \approx - \frac{K}{2K_0} \vec{v}_i.
\end{displaymath}
Since, the total energy $K + E_{pot}$ is constant the change in the kinetic
energy corresponds to a change of the potential energy
\begin{displaymath}
\delta K = - \delta E_{pot}.  
\end{displaymath}
The change in the velocity can therefore be written as
\begin{displaymath}
  \delta \vec{v}_i \approx - \zeta_G \vec{v}_i \delta t,
\end{displaymath}
where 
\begin{displaymath}
  \zeta_G = - \frac{1}{2K_0} \frac{E_{pot}}{\delta t}.
\end{displaymath}
The parameter $\zeta_G$ is proportional to the change of the potential energy
in a time step. In the next subsection, we will see that the simple rescaling
algorithm introduced here may be given a more profound theoretical foundation.

\subsection{The Gaussian thermostat}
In 1929 Gauss formulated a general principle of mechanics for the 
description of systems with holonom and nonholonom constraints. 
The Gaussian principle of least Zwang introduces as a measure of the constraint
\begin{displaymath}
  Z = \sum_k m_k (\ddot{\vec{x}}_k - \vec{F}_k )^2,
\end{displaymath}
where, of course,
\begin{displaymath}
  \ddot{\vec{x}}_k = \vec{F}_k
\end{displaymath}
describes the free motion. In the Gaussian notation the quantity $Z$ is called
the sum over the squared of the "lost forces". The constraint equations of
motion are obtained from the variational principle
\begin{displaymath}
  \delta Z =0.
\end{displaymath}
During the variation of the functional $Z$ we have to keep fixed (i) 
the state of the system
\begin{displaymath}
  \delta \vec{x}_k =0; \;\;\; \delta \dot{\vec{x}}_k =0,
\end{displaymath}
(ii) the (nonholonomic) constraint
\begin{displaymath}
f_i(\vec{x}_1, \ldots, \vec{x}_N; \dot{\vec{x}}_1, \ldots, \dot{\vec{x}}_N);
 \;\;\; i=1, \ldots, r  
\end{displaymath}
and (iii) the forces acting on the system and the masses
\begin{displaymath}
  \delta \vec{F}_k =0; \;\;\; \delta m_k =0.
\end{displaymath}
By the method of Lagrangian multipliers we obtain
\begin{displaymath}
  \delta Z = 2 \sum_k ( m_k \ddot{\vec{x}}_k - \vec{F}_k - 
                        \sum_{i=1}^r \lambda_i 
                        \frac{\partial f_i}{\partial \vec{x}_k}
                       ) \delta \ddot{\vec{x}}_k =0.
\end{displaymath}
Note that we made use of the fact that it follows from
\begin{equation}
\label{eq:GaussMultip}
  \sum_k \left\{ \frac{\partial f_i}{\partial \vec{x}_k}  \delta \vec{x}_k
              + \frac{\partial f_i}{\partial \dot{\vec{x}}_k} 
                       \delta \dot{\vec{x}}_k \right\} =0.
\end{equation}
that
\begin{displaymath}
  \sum_k \frac{\partial f_i}{\partial \dot{\vec{x}}}_k \delta \ddot{\vec{x}}_k
          =0.
\end{displaymath}
It follows now from Eq. (\ref{eq:GaussMultip}) that the equations of motion 
read
\begin{equation}
\label{eq:GaussEq}
  m_k \ddot{\vec{x}}_k = \vec{F}_k + \sum_{i=1}^r \lambda_i 
                   \frac{\partial f_i}{\partial \dot{\vec{x}}_k}.
\end{equation}

Let us now apply the Gaussian principle to a dynamical system which we want to
keep at a fixed temperature. The constraint is
\begin{equation}
\label{eq:GaussConstraintT}
  f(\dot{\vec{x}}_1, \ldots,\dot{\vec{x}}_N) =
         \sum_{i=1}^N \frac{1}{2} m_i \dot{\vec{x}}_i^2 - \frac{3}{2}NkT_0 =0.
\end{equation}
Since,
\begin{displaymath}
  \frac{\partial f}{\partial \dot{\vec{x}}_i} =  m_i \dot{\vec{x}}_i
\end{displaymath}
the constraint equation of motion follows immediately from 
the Gaussian principle
(Eq. (\ref{eq:GaussEq}))
\begin{equation}
  \label{eq:GaussThermo1}
   m_k \ddot{\vec{x}}_k = \vec{F}_k + \lambda m_k \dot{\vec{x}}_k.
\end{equation}
In order to determine the Langrange multiplier $\lambda$ we take the time
derivative of the constraint (\ref{eq:GaussConstraintT}) and find
\begin{equation}
\label{eq:GaussThermo3}
  \sum_i m_i \dot{\vec{x}}_i \cdot \ddot{\vec{x}}_i =0.
\end{equation}
Multiplying Eq. (\ref{eq:GaussThermo1}) by $\vec{x}_k$ and summing over all
particles  we get
\begin{displaymath}
  \sum_k m_k \dot{\vec{x}}_k \cdot \ddot{\vec{x}}_k = 
      \sum_k  \dot{\vec{x}}_k \cdot\vec{F}_k + 
           \lambda \sum_k m_k \dot{\vec{x}}_k^2.
\end{displaymath}
Exploiting Eq. (\ref{eq:GaussThermo3}) we  obtain the Lagrangian
multiplier $\lambda$
\begin{equation}
\label{eq:gaussianThermo4}
  \lambda = - \frac{\sum_k  \dot{\vec{x}}_k \cdot\vec{F}_k}
                   {\sum_k m_k \dot{\vec{x}}_k^2}
\end{equation}
The equations of motion thermalized with the Gaussian "thermostat" finally
read
\begin{displaymath}
  m_k \ddot{\vec{x}}_k = \vec{F}_k - \zeta_G m_k \dot{\vec{x}}_k
\end{displaymath}
where the constant factor $\zeta_G = - \lambda$.
The equivalence of the above thermostat and of the rescaling of the velocities
is now easily recognised. It follows immediately from Eq. 
(\ref{eq:gaussianThermo4})  that
\begin{displaymath}
2 K \zeta_G = - \frac{d E_{pot}}{dt}.    
\end{displaymath}
Thus, if at the beginning of the simulation the desired kinetic energy is
$K_0$ then, the rescaling the velocities is equivalent to
modifying the equations of motion according to the Gaussian principle.

\section{Non--Equilibrium Molecular Dynamics}
So far we have considered systems at equilibrium. Non--equilibrium molecular
dynamics (NEMD) is a generalisation of molecular dynamics which tries to
investigate non equilibrium properties of fluids \cite{EvansNEMD}. 
A typical non equilibrium
situation is plane Couette flow. A simple shear flow in the $x$ direction with
the velocity gradient in the $y$ direction 
is characterized by the shear rate $\gamma$ which is given by
\begin{displaymath}
  \gamma = \frac{\partial v_x}{\partial y}.
\end{displaymath}
In other words the velocity gradient is of the form
\begin{displaymath}
  \vec{v}(\vec{r}) = \gamma y \hat{n}_x,
\end{displaymath}
where $\hat{n}_x$ is the unit vector in the $x$--direction. Such a steady
uniform shearing motion 
may be imagined to be driven by moving boundaries normal to the $y$
axis  at $y = \pm \infty$. The velocity field
of a plane Couette flow is depicted in Fig. (\ref{Fig:CouetteFlow}).

\begin{figure}[htbp]
  \begin{center}
    
    \caption{The velocity field in a plane Couette flow.}
    \label{Fig:CouetteFlow}
  \end{center}
\end{figure}

A typical question to be answered by a NEMD simulation could be the following
one: Does the viscosity $\eta$ of the fluid depend on the shear rate? We will
not discuss here the theoretical foundations of this question. Quantities like
the viscosity  may be calculated directly by evaluating the corresponding
kinetic expressions or with the help of Green--Kubo formulas. Here, we want
only to sketch how a NEMD simulation may be performed \cite{Evans}.
A peculiarity of the driven Couette flow is that it is possible to design an
algorithm which uses only the boundary conditions to drive the
non--equilibrium state.

Again we consider a cubic simulation cell with periodic boundary conditions.
At the   origin of the simulation cube the streaming velocity is chosen to be
zero, 
\begin{displaymath}
  \vec{u}(\vec{0}) = 0.
\end{displaymath}
It is important to be aware of the fact that in a flow the velocity of a
particle, say $i$, is the sum of two contributions: a peculiar velocity
$\vec{c}_i$  and a streaming velocity
\begin{displaymath}
  \dot{\vec{r}}_i = \vec{c}_i + \vec{u}(\vec{r}_i).
\end{displaymath}
Of course, the peculiar velocity is periodic, while the streaming velocity is
not! 

At time $t=0$, i.e. before the shear flow is switched on, we have the usual
periodic boundary condition
\begin{displaymath}
  \vec{r}_i = (\vec{r}_i) \mathrm{mod}L.
\end{displaymath}
Simple shear flow may be be generated by moving image particles undergoing an
ideal Couette flow with the prescribed shear rate. Let us switch on the flow 
at time $t=0$. Then, at a later time $t$ the image cells above (below)  the
simulation box have moved in the $x$ direction to the right (left) by a
distance $(\gamma tL) \mathrm{mod} L$ where $L$ is the linear dimension of the
cubic box (see Fig. (\ref{fig:NEMD_Periodic})).

\begin{figure}[htbp]
  \begin{center}
    
    \caption{Moving periodic images for the simulation of a plane Couette flow.}
    \label{fig:NEMD_Periodic}
  \end{center}
\end{figure}

Formally we have the following situation. Particle $i$ and its two images
$i'$ and $i''$ are located at time $t_1$ at the positions 
\begin{displaymath}
  \vec{r}'_i(t_1) = \int_0^{t_1} dt 
                    \left( \vec{c}_i + \gamma y_i \hat{n}_x 
                    \right) = \vec{r}_i + \gamma L \hat{n}_x t_1
\end{displaymath}
and, respectively,
\begin{displaymath}
  \vec{r}''_i(t_1) = \vec{r}_i - \gamma L \hat{n}_x t_1.
\end{displaymath}
Thus, the method of imaging particles in a system under shear is easily
formulated. If particle $i$ passes through either face of the cube which is
parallel to the $y$--axis, the periodic boundary conditions are unchanged
\begin{displaymath}
  \vec{r}^{\mathrm{new}}_i = (\vec{r}_i) \mathrm{mod} L.
\end{displaymath}
If particle $i$ passes through either face which is parallel to the $x$ axis
then it is replaced in the simulation cube by one of its images $i'$ or $i''$.
If particle $i$ passes through the top face ($y=L)$ then its coordinate
$\vec{r}_i$ is replaced by $\vec{r}{''}_i$,
\begin{displaymath}
  \vec{r}^{\mathrm{new}}_i = (\vec{r}{''}_i) \mathrm{mod} L 
                           = (\vec{r}_i - \gamma L \hat{n}_x t_1) 
                           \mathrm{mod} L.
\end{displaymath}
Obviously, the streaming velocity at $\vec{r}_i$ is different to that at
$\vec{r}_i''$, so we have to correct the velocity accordingly and 
$\dot{\vec{r}}_i$ becomes
\begin{displaymath}
  \dot{\vec{r}}^{\mathrm{new}}_i = \dot{\vec{r}}_i''
                            = \dot{\vec{r}}_i - \gamma L \hat{n}_x.
\end{displaymath}
Analogously, if the particle $i$ passes through the bottom face
of the simulation cube ($y=0$) then $\vec{r}_i$ is replaced by
$\vec{r}_i'$
\begin{displaymath}
  \vec{r}^{\mathrm{new}}_i = (\vec{r}{'}_i) \mathrm{mod} L 
                           = (\vec{r}_i + \gamma L \hat{n}_x t_1) 
                           \mathrm{mod} L.
\end{displaymath}
and its velocity becomes
\begin{displaymath}
  dot{\vec{r}}^{\mathrm{new}}_i = \dot{\vec{r}}_i'
                            = \dot{\vec{r}}_i + \gamma L \hat{n}_x
\end{displaymath}
These are the so--called "sliding brick" periodic boundary conditions.

There are also alternative methods to simulate Couette flow. These methods are
based on the use of shearing periodic boundary conditions as well as on 
Non--Newtonian equations of motion \index{Non--Newtonian equations of motion}.
They can be applied to shear flows with
time dependent shear rates. The discussion of these more refined simulation
techniques (SLLOD dynamics) \index{SLLOD dynamics}
\cite{Allen} are beyond the sope of the present book. 

With the help of the above NEMD algorithm it is possible to calculate the
(Non--Newtonian) viscosity \index{viscosity!Non--Newtonian} of 
the fluid from the Cartesian components of the
stress tensor \index{stress tensor} $\sigma_{\mu \nu} = - p_{\mu \nu}$ or
of the pressure tensure \index{pressure tensor} $p_{\mu \nu}$. The latter is
the sum of the kinetic and the potential contributions:
\begin{eqnarray*}
  p_{\mu \nu} & = & p_{\mu \nu}^{\mathrm{kin}} +p_{\mu \nu}^{\mathrm{pot}}, \\
  V  p_{\mu \nu}^{\mathrm{kin}} = \sum_{i=1}^N m_i c^i_{\mu} c^i_{\nu} \\
  V p_{\mu \nu}^{\mathrm{pot}} = \frac{1}{2} \sum_{ij} 
                    r_{\mu}^{ij} F_{\nu}^{ij}. 
\end{eqnarray*}
In the above equations $\vec{r}^{ij} = \vec{r}_i - \vec{r}_j$ is the relative
position vector of particles $i$ and $j$ and $\vec{F}^{ij}$ is the force
acting between them. The Greek subscripts $\mu$, $\nu$ assume the values
$x$, $y$, $z$ and denote the Cartesian components of the corresponding
vectors. For the flow geometry we consider here the (non--Newtonian) viscosity
$\eta$ is obtained through
\begin{displaymath}
  \eta = \frac{\sigma_{xy}}{\gamma} = -\frac{p_{xy}}{\gamma}.
\end{displaymath}
In the simulation the kinetic and the potential contribution to the pressure
tensor and to the viscosity can be computed separately. Usually it is
necessary to estimate them as time averages over $10^3$ to $10^6$ time steps.
For dense fluids the potential contribution will be the dominant terms,
whereas for dilute gases the kinetic contribution will be the more important. 
It is interesting to look at the shear--rate dependence of the viscosity. Four
regimes can be identified. In the first {\textit{Newtonian}} regime
at low shear rates ($\gamma < 0.1$ in Lennard--Jones units) the 
viscosity is independent from the shear rate. For $0.2 \gamma <2$ a week
\textit{shear thinning} \index{shear thinning} is observed. 
This means that the viscosity decreases
for increasing shear rate. At larger shear rates for $2 < \gamma < 20$ a
\textit{strong shear thinning} is observed. And finally for even larger shear
rates $\gamma >20$ \textit{shear thickening} \index{shear thickening} 
is observed, i.e., the viscosity increases with increasing shear rate
\cite{Hess}. 


Code der Simulation: Nur periodic boundary conditions; Trajektorien anschauen
fuer grosse Scherraten; g(r); usw.

Appendix: The GUI!!!




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{peter}
\bibliography{V_98,simulit}







%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "V_98"
%%% End: 
